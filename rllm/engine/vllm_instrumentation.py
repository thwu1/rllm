"""vLLM instrumentation for token IDs support (vLLM < 0.10.2).

This module provides monkey patching to add token IDs to vLLM's
OpenAI-compatible API responses for versions before 0.10.2, matching the
native vLLM 0.10.2+ format.

Native vLLM 0.10.2+ format:
- prompt_token_ids: list[int] at top level (for the prompt)
- token_ids: list[int] per choice (for response tokens)

For vLLM >= 0.10.2, use the native `return_token_ids=True` parameter instead.

Example:
    >>> from rllm.engine.vllm_instrumentation import instrument_vllm
    >>> instrument_vllm()
    >>> # Now all vLLM responses include token IDs in native format
"""

from __future__ import annotations

import logging
from typing import Any

logger = logging.getLogger(__name__)

__all__ = [
    "instrument_vllm",
    "uninstrument_vllm",
    "is_vllm_instrumented",
]


# Store original references and state
_original_chat_completion_full_generator = None
_original_response_class = None
_is_instrumented = False


def _check_vllm_available() -> bool:
    """Check if vLLM is available."""
    try:
        import vllm  # noqa: F401

        return True
    except ImportError:
        return False


def _get_vllm_version() -> tuple[int, int, int] | None:
    """Get vLLM version as tuple."""
    try:
        import vllm

        version_str = vllm.__version__
        # Parse version like "0.9.1" or "0.10.2"
        parts = version_str.split(".")[:3]
        return tuple(int(p) for p in parts)
    except Exception:
        return None


def instrument_vllm(force: bool = False) -> bool:
    """Instrument vLLM to capture token IDs generated by engine.

    This instrumentation adds token ID fields to vLLM's ChatCompletionResponse
    for versions < 0.10.2, matching the native vLLM 0.10.2+ format:
    - prompt_token_ids at top level (for the prompt)
    - token_ids per choice (for response tokens)

    For vLLM >= 0.10.2, this instrumentation is not needed as the feature is
    natively supported via the `return_token_ids=True` parameter.

    Args:
        force: If True, apply instrumentation even if vLLM >= 0.10.2.
               Default is False (auto-detect version).

    Returns:
        True if instrumentation was applied, False otherwise.

    Example:
        >>> from rllm.engine.vllm_instrumentation import instrument_vllm
        >>> instrument_vllm()
        >>> # Now all vLLM responses include token IDs in native format
    """
    # Force import vLLM modules to ensure they're loaded before patching
    try:
        logger.info("[VLLM_INSTRUMENT] Force importing vLLM modules...")
        import vllm
        import vllm.entrypoints.openai.protocol
        import vllm.entrypoints.openai.serving_chat

        logger.info(f"[VLLM_INSTRUMENT] vLLM modules imported successfully, vllm.__version__={vllm.__version__}")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è [VLLM_INSTRUMENT] vLLM not available: {e}")
        return False

    if not _check_vllm_available():
        logger.warning("‚ö†Ô∏è [VLLM_INSTRUMENT] vLLM not available, skipping instrumentation")
        return False

    # Check version
    version = _get_vllm_version()
    logger.info(f"üîç [VLLM_INSTRUMENT] vLLM version detected: {version}")
    if version and version >= (0, 10, 2) and not force:
        msg = f"vLLM {'.'.join(map(str, version))} has native return_token_ids support. Instrumentation not needed."
        logger.info(f"‚ÑπÔ∏è [VLLM_INSTRUMENT] {msg}")
        return False

    try:
        import vllm.entrypoints.openai.protocol
        from vllm.entrypoints.openai.protocol import ChatCompletionResponse
        from vllm.entrypoints.openai.serving_chat import OpenAIServingChat

        # Check if already instrumented
        global _is_instrumented, _original_chat_completion_full_generator, _original_response_class

        if _is_instrumented:
            logger.warning("‚ö†Ô∏è [VLLM_INSTRUMENT] vLLM is already instrumented by RLLM. Skipping.")
            return False

        logger.info("üîß [VLLM_INSTRUMENT] Starting vLLM instrumentation...")

        # Store original references
        _original_chat_completion_full_generator = OpenAIServingChat.chat_completion_full_generator
        _original_response_class = ChatCompletionResponse

        logger.info(f"üì¶ [VLLM_INSTRUMENT] Stored original chat_completion_full_generator: {_original_chat_completion_full_generator}")
        logger.info(f"üì¶ [VLLM_INSTRUMENT] Stored original ChatCompletionResponse: {_original_response_class}")

        # Import ChatCompletionResponseChoice to patch it
        from vllm.entrypoints.openai.protocol import ChatCompletionResponseChoice

        # Create patched choice class with token_ids per choice
        # (matching native vLLM 0.10.2+ format)
        class ChatCompletionResponseChoicePatched(ChatCompletionResponseChoice):
            """Extended ChatCompletionResponseChoice with token IDs matching native vLLM 0.10.2+ format."""

            # Native vLLM 0.10.2+ has token_ids per choice (for response tokens)
            token_ids: list[int] | None = None

        # Create patched response class with prompt_token_ids at top level
        # (matching native vLLM 0.10.2+ format)
        class ChatCompletionResponsePatched(ChatCompletionResponse):
            """Extended ChatCompletionResponse with token IDs matching native vLLM 0.10.2+ format."""

            # Native vLLM 0.10.2+ has prompt_token_ids at top level
            prompt_token_ids: list[int] | None = None
            choices: list[ChatCompletionResponseChoicePatched]

        # Create patched generator
        async def chat_completion_full_generator(
            self: Any,
            request: Any,
            result_generator: Any,
            request_id: str,
            model_name: str,
            conversation: Any,
            tokenizer: Any,
            request_metadata: Any,
        ) -> Any:
            """Patched generator that extracts token IDs and adds them in native vLLM 0.10.2+ format."""
            logger.debug(f"üéØ [VLLM_INSTRUMENT] Patched generator called for request_id={request_id}")
            prompt_token_ids: list[int] | None = None
            response_token_ids: list[list[int]] | None = None

            async def _generate_interceptor():
                """Intercept generator to extract token IDs."""
                nonlocal prompt_token_ids, response_token_ids
                async for res in result_generator:
                    yield res
                    # Extract token IDs from vLLM's internal RequestOutput
                    prompt_token_ids = res.prompt_token_ids
                    response_token_ids = [output.token_ids for output in res.outputs]
                    logger.debug(f"üîç [VLLM_INSTRUMENT] Extracted token IDs: prompt={len(prompt_token_ids) if prompt_token_ids else 0}, response={len(response_token_ids) if response_token_ids else 0} choices")

            # Call original generator with interceptor
            response = await _original_chat_completion_full_generator(
                self,
                request,
                _generate_interceptor(),
                request_id,
                model_name,
                conversation,
                tokenizer,
                request_metadata,
            )

            # Add token IDs to response in native vLLM 0.10.2+ format:
            # - prompt_token_ids at top level (for the prompt)
            # - token_ids per choice (for response tokens)
            logger.debug(f"üìù [VLLM_INSTRUMENT] Adding token IDs to response: prompt={len(prompt_token_ids) if prompt_token_ids else 0}, response={len(response_token_ids) if response_token_ids else 0} choices")

            # Update choices to add response token_ids directly to choice (matching native vLLM format)
            updated_choices = []
            for i, choice in enumerate(response.choices):
                # Get response token IDs for this choice
                choice_token_ids = response_token_ids[i] if response_token_ids and i < len(response_token_ids) else []

                # Native vLLM 0.10.2+ adds token_ids directly to the choice (for response tokens)
                updated_choice = choice.model_copy(update={"token_ids": choice_token_ids})
                updated_choices.append(updated_choice)

            # Update response with prompt_token_ids at top level and updated choices
            response = response.model_copy(
                update={
                    "prompt_token_ids": prompt_token_ids,
                    "choices": updated_choices,
                }
            )

            logger.debug(f"‚úÖ [VLLM_INSTRUMENT] Response has token IDs: prompt_token_ids={len(response.prompt_token_ids) if response.prompt_token_ids else 0}, choices[0].token_ids={len(response.choices[0].token_ids) if response.choices and response.choices[0].token_ids else 0}")

            return response

        # Apply patches
        logger.info("üî® [VLLM_INSTRUMENT] Applying patches to vLLM...")
        vllm.entrypoints.openai.protocol.ChatCompletionResponse = ChatCompletionResponsePatched
        OpenAIServingChat.chat_completion_full_generator = chat_completion_full_generator
        _is_instrumented = True

        logger.info("‚úÖ [VLLM_INSTRUMENT] vLLM instrumented successfully to return token IDs")
        logger.info(f"üìä [VLLM_INSTRUMENT] Patched ChatCompletionResponse: {ChatCompletionResponsePatched}")

        logger.info(f"üìä [VLLM_INSTRUMENT] Patched generator: {chat_completion_full_generator}")
        return True

    except Exception as e:
        logger.error(f"‚ùå [VLLM_INSTRUMENT] Failed to instrument vLLM: {e}")
        import traceback

        traceback.print_exc()
        return False


def uninstrument_vllm() -> bool:
    """Uninstrument vLLM to stop capturing token IDs.

    This restores vLLM's original behavior.

    Returns:
        True if uninstrumentation was successful, False otherwise.
    """
    if not _check_vllm_available():
        logger.warning("vLLM not available, nothing to uninstrument")
        return False

    try:
        import vllm.entrypoints.openai.protocol
        from vllm.entrypoints.openai.serving_chat import OpenAIServingChat

        global _original_chat_completion_full_generator, _original_response_class, _is_instrumented

        if not _is_instrumented:
            logger.warning("vLLM was not instrumented by RLLM")
            return False

        # Restore original generator
        OpenAIServingChat.chat_completion_full_generator = _original_chat_completion_full_generator

        # Restore original response class
        if _original_response_class is not None:
            vllm.entrypoints.openai.protocol.ChatCompletionResponse = _original_response_class

        # Reset state
        _original_chat_completion_full_generator = None
        _original_response_class = None
        _is_instrumented = False

        logger.info("vLLM uninstrumented successfully")
        return True

    except Exception as e:
        logger.error(f"Failed to uninstrument vLLM: {e}")
        return False


def is_vllm_instrumented() -> bool:
    """Check if vLLM is currently instrumented.

    Returns:
        True if vLLM is instrumented, False otherwise.
    """
    return _is_instrumented


def get_vllm_token_ids_support() -> str:
    """Get information about vLLM token IDs support.

    Returns:
        One of:
        - "native": vLLM >= 0.10.2 with native support
        - "instrumented": vLLM < 0.10.2 with RLLM instrumentation
        - "none": vLLM < 0.10.2 without instrumentation
        - "unavailable": vLLM not installed
    """
    if not _check_vllm_available():
        return "unavailable"

    version = _get_vllm_version()
    if version and version >= (0, 10, 2):
        return "native"

    if is_vllm_instrumented():
        return "instrumented"

    return "none"


def check_vllm_instrumentation_status() -> dict:
    """Check detailed vLLM instrumentation status in current process.

    This is useful for debugging whether the instrumentation is present in Ray workers.

    Returns:
        Dictionary with instrumentation status details
    """
    status = {
        "vllm_available": _check_vllm_available(),
        "vllm_version": None,
        "is_instrumented_flag": _is_instrumented,
        "has_patched_generator": False,
        "has_patched_response_class": False,
    }

    if not status["vllm_available"]:
        return status

    try:
        import vllm

        status["vllm_version"] = vllm.__version__

        from vllm.entrypoints.openai.protocol import ChatCompletionResponse
        from vllm.entrypoints.openai.serving_chat import OpenAIServingChat

        # Check if the generator is patched by looking at its name or checking if it's our function
        generator = OpenAIServingChat.chat_completion_full_generator
        if generator is not None:
            # Our patched function will have specific attributes or name
            status["generator_name"] = getattr(generator, "__name__", str(generator))
            status["has_patched_generator"] = "Patched generator that extracts token IDs" in str(generator.__doc__ or "")

        # Check if response class has our custom fields
        status["has_patched_response_class"] = hasattr(ChatCompletionResponse, "__annotations__") and "prompt_token_ids" in getattr(ChatCompletionResponse, "__annotations__", {})

    except Exception as e:
        status["error"] = str(e)

    logger.info(f"[VLLM_INSTRUMENT] Instrumentation status check: {status}")
    return status
