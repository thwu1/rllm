{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLLM SDK Tutorial: From Rollouts to Training\n",
    "\n",
    "This tutorial demonstrates the complete RLLM SDK workflow:\n",
    "\n",
    "1. **Setup**: Start proxy manager and configure OpenAI\n",
    "2. **Simple Rollout**: Execute a basic rollout function\n",
    "3. **Trace Retrieval**: Fetch traces from SQLite storage\n",
    "4. **Trajectory Visualization**: Print and visualize trajectories\n",
    "5. **Solver-Judge Pattern**: Advanced workflow with decorators\n",
    "6. **SFT Training**: Supervised fine-tuning on collected traces\n",
    "7. **RL Training**: Reinforcement learning with PPO\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "- OpenAI API key set: `export OPENAI_API_KEY=\"sk-...\"`\n",
    "- RLLM installed: `pip install -e .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import click\n",
    "\n",
    "from rllm.sdk import (\n",
    "    get_chat_client_async,\n",
    "    session,\n",
    "    trajectory,\n",
    "    TrajectoryView,\n",
    ")\n",
    "from rllm.sdk.proxy.proxy_manager import ProxyManager\n",
    "from rllm.sdk.store import SqliteTraceStore\n",
    "\n",
    "# Check for OpenAI API key\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Printing Utility\n",
    "\n",
    "We'll use this helper function for colorful output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorful_print(string: str, *args, **kwargs) -> None:\n",
    "    \"\"\"Print colored text using click.\"\"\"\n",
    "    end = kwargs.pop(\"end\", \"\\n\")\n",
    "    print(click.style(string, *args, **kwargs), end=end, flush=True)\n",
    "\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print a section header.\"\"\"\n",
    "    colorful_print(\"\\n\" + \"=\" * 70, fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\" {title}\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "\n",
    "\n",
    "print_section(\"Color Printing Utility Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Proxy Manager\n",
    "\n",
    "The ProxyManager automatically starts a LiteLLM proxy server that handles:\n",
    "- OpenAI API calls\n",
    "- Automatic trace collection\n",
    "- Persistent storage in SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DB_PATH = \"/tmp/rllm_tutorial.db\"\n",
    "PROXY_PORT = 4000\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Clean up existing database\n",
    "db_file = Path(DB_PATH)\n",
    "if db_file.exists():\n",
    "    db_file.unlink()\n",
    "    print(f\"✓ Removed existing database: {DB_PATH}\")\n",
    "\n",
    "# Create OpenAI configuration\n",
    "config = {\n",
    "    \"model_list\": [\n",
    "        {\n",
    "            \"model_name\": MODEL,\n",
    "            \"litellm_params\": {\n",
    "                \"model\": MODEL,\n",
    "                \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create and start proxy manager\n",
    "proxy_manager = ProxyManager(\n",
    "    proxy_host=\"127.0.0.1\",\n",
    "    proxy_port=PROXY_PORT,\n",
    "    admin_token=\"tutorial-admin-token\",\n",
    ")\n",
    "\n",
    "# Start proxy subprocess\n",
    "config_path = proxy_manager.start_proxy_subprocess(\n",
    "    config=config,\n",
    "    db_path=DB_PATH,\n",
    "    project=\"tutorial-project\",\n",
    ")\n",
    "\n",
    "proxy_url = proxy_manager.get_proxy_url(include_v1=True)\n",
    "print_section(\"Proxy Manager Started\")\n",
    "colorful_print(f\"Proxy URL: {proxy_url}\", fg=\"green\")\n",
    "colorful_print(f\"Database: {DB_PATH}\", fg=\"green\")\n",
    "colorful_print(f\"Model: {MODEL}\", fg=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Dataset\n",
    "\n",
    "We'll create a tiny math dataset for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple math problems\n",
    "MATH_PROBLEMS = [\n",
    "    {\n",
    "        \"question\": \"What is 15 + 27?\",\n",
    "        \"ground_truth\": \"42\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If a rectangle has length 8 and width 5, what is its area?\",\n",
    "        \"ground_truth\": \"40\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the square root of 144?\",\n",
    "        \"ground_truth\": \"12\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print_section(\"Dataset Created\")\n",
    "for i, problem in enumerate(MATH_PROBLEMS, 1):\n",
    "    colorful_print(f\"Problem {i}: {problem['question']}\", fg=\"yellow\")\n",
    "    colorful_print(f\"  Answer: {problem['ground_truth']}\", fg=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Extract numerical answer from text.\"\"\"\n",
    "    # Look for numbers in the text\n",
    "    numbers = re.findall(r'\\b\\d+\\.?\\d*\\b', text)\n",
    "    return numbers[-1] if numbers else \"\"\n",
    "\n",
    "\n",
    "def simple_math_reward(response: str, ground_truth: str) -> float:\n",
    "    \"\"\"Simple exact match reward.\"\"\"\n",
    "    predicted = extract_answer(response)\n",
    "    return 1.0 if predicted == ground_truth else 0.0\n",
    "\n",
    "\n",
    "# Test the reward function\n",
    "test_response = \"The answer is 42\"\n",
    "test_gt = \"42\"\n",
    "reward = simple_math_reward(test_response, test_gt)\n",
    "colorful_print(f\"\\nTest reward: {reward}\", fg=\"green\" if reward == 1.0 else \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Rollout Function\n",
    "\n",
    "This is the simplest version - similar to `train_hendrycks_math.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def simple_rollout(question: str, ground_truth: str) -> tuple[str, float, str]:\n",
    "    \"\"\"\n",
    "    Execute a simple rollout.\n",
    "    \n",
    "    Args:\n",
    "        question: The math problem\n",
    "        ground_truth: The correct answer\n",
    "    \n",
    "    Returns:\n",
    "        (session_uid, reward, response_text)\n",
    "    \"\"\"\n",
    "    client = get_chat_client_async(base_url=proxy_url, api_key=\"EMPTY\")\n",
    "    \n",
    "    with session(task=\"simple_math\") as sess:\n",
    "        session_uid = sess._uid\n",
    "        \n",
    "        # Make LLM call\n",
    "        response = await client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = simple_math_reward(response_text, ground_truth)\n",
    "        \n",
    "        return session_uid, reward, response_text\n",
    "\n",
    "\n",
    "print_section(\"Simple Rollout Function Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Executing Rollouts\")\n",
    "\n",
    "rollout_results = []\n",
    "\n",
    "for i, problem in enumerate(MATH_PROBLEMS, 1):\n",
    "    colorful_print(f\"\\nRollout {i}/{len(MATH_PROBLEMS)}\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\"Question: {problem['question']}\", fg=\"yellow\")\n",
    "    \n",
    "    session_uid, reward, response = await simple_rollout(\n",
    "        problem[\"question\"],\n",
    "        problem[\"ground_truth\"]\n",
    "    )\n",
    "    \n",
    "    rollout_results.append({\n",
    "        \"session_uid\": session_uid,\n",
    "        \"question\": problem[\"question\"],\n",
    "        \"ground_truth\": problem[\"ground_truth\"],\n",
    "        \"response\": response,\n",
    "        \"reward\": reward,\n",
    "    })\n",
    "    \n",
    "    colorful_print(f\"Response: {response}\", fg=\"white\")\n",
    "    reward_color = \"green\" if reward == 1.0 else \"red\"\n",
    "    colorful_print(f\"Reward: {reward}\", fg=reward_color, bold=True)\n",
    "    colorful_print(f\"Session UID: {session_uid}\", fg=\"blue\")\n",
    "\n",
    "avg_reward = sum(r[\"reward\"] for r in rollout_results) / len(rollout_results)\n",
    "colorful_print(f\"\\n✓ Average Reward: {avg_reward:.2f}\", fg=\"green\", bold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetch Traces from SQLite Store\n",
    "\n",
    "After executing rollouts, we flush the tracer and retrieve traces from SQLite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Flushing and Retrieving Traces\")\n",
    "\n",
    "# Flush tracer to ensure all traces are written to database\n",
    "flush_result = await proxy_manager.flush_tracer(timeout=30.0)\n",
    "colorful_print(f\"✓ Flush result: {flush_result}\", fg=\"green\")\n",
    "\n",
    "# Create store and retrieve traces\n",
    "store = SqliteTraceStore(db_path=DB_PATH)\n",
    "\n",
    "all_traces = []\n",
    "for result in rollout_results:\n",
    "    session_uid = result[\"session_uid\"]\n",
    "    traces = await store.get_by_session_uid(session_uid)\n",
    "    \n",
    "    colorful_print(f\"\\nSession {session_uid}:\", fg=\"cyan\")\n",
    "    colorful_print(f\"  Found {len(traces)} trace(s)\", fg=\"yellow\")\n",
    "    \n",
    "    for trace in traces:\n",
    "        colorful_print(f\"    Trace ID: {trace.id}\", fg=\"blue\")\n",
    "        all_traces.append((trace, result))\n",
    "\n",
    "colorful_print(f\"\\n✓ Total traces retrieved: {len(all_traces)}\", fg=\"green\", bold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trajectory Visualization\n",
    "\n",
    "Let's create a nice visualization function for trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trajectory(trace, metadata: dict):\n",
    "    \"\"\"\n",
    "    Print a trajectory with color-coded information.\n",
    "    \n",
    "    Args:\n",
    "        trace: Trace object from SQLite store\n",
    "        metadata: Additional metadata (question, reward, etc.)\n",
    "    \"\"\"\n",
    "    data = trace.data\n",
    "    \n",
    "    colorful_print(\"\\n\" + \"=\" * 70, fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\"TRAJECTORY: {trace.id}\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "    \n",
    "    # Session info\n",
    "    colorful_print(f\"Session UID: {metadata['session_uid']}\", fg=\"blue\")\n",
    "    \n",
    "    # Model info\n",
    "    colorful_print(f\"Model: {data['model']}\", fg=\"white\")\n",
    "    colorful_print(f\"Latency: {data['latency_ms']:.2f} ms\", fg=\"white\")\n",
    "    \n",
    "    # Token usage\n",
    "    tokens = data[\"tokens\"]\n",
    "    colorful_print(\n",
    "        f\"Tokens: prompt={tokens['prompt']}, completion={tokens['completion']}, total={tokens['total']}\",\n",
    "        fg=\"white\"\n",
    "    )\n",
    "    \n",
    "    # Input\n",
    "    colorful_print(\"\\nINPUT:\", fg=\"yellow\", bold=True)\n",
    "    input_messages = data[\"input\"][\"messages\"]\n",
    "    for msg in input_messages:\n",
    "        role_color = \"green\" if msg[\"role\"] == \"user\" else \"blue\"\n",
    "        colorful_print(f\"  [{msg['role']}]: {msg['content']}\", fg=role_color)\n",
    "    \n",
    "    # Output\n",
    "    colorful_print(\"\\nOUTPUT:\", fg=\"magenta\", bold=True)\n",
    "    output_choices = data[\"output\"][\"choices\"]\n",
    "    output_text = output_choices[0][\"message\"][\"content\"]\n",
    "    colorful_print(f\"  {output_text}\", fg=\"white\")\n",
    "    \n",
    "    # Ground truth and reward\n",
    "    colorful_print(\"\\nEVALUATION:\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\"  Ground Truth: {metadata['ground_truth']}\", fg=\"green\")\n",
    "    \n",
    "    reward = metadata[\"reward\"]\n",
    "    reward_color = \"green\" if reward == 1.0 else \"red\"\n",
    "    reward_symbol = \"✓\" if reward == 1.0 else \"✗\"\n",
    "    colorful_print(f\"  Reward: {reward_symbol} {reward}\", fg=reward_color, bold=True)\n",
    "    \n",
    "    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "\n",
    "\n",
    "print_section(\"Trajectory Visualization Function Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize All Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Visualizing Trajectories\")\n",
    "\n",
    "for trace, metadata in all_traces:\n",
    "    print_trajectory(trace, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Solver-Judge Pattern with Decorators\n",
    "\n",
    "Now let's demonstrate a more complex workflow using the `@trajectory` decorator.\n",
    "This shows how RLLM SDK automatically formats trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathSolver:\n",
    "    \"\"\"Solver that generates multiple solutions.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, model: str):\n",
    "        self.client = get_chat_client_async(base_url=base_url, api_key=\"EMPTY\")\n",
    "        self.model = model\n",
    "    \n",
    "    @trajectory(name=\"solver\")\n",
    "    async def generate_solution(self, problem: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a solution using @trajectory decorator.\n",
    "        \n",
    "        The decorator:\n",
    "        - Creates a session internally\n",
    "        - Tracks LLM calls automatically\n",
    "        - Returns TrajectoryView with result field set to the return value\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{problem}\\n\\nPlease solve this problem and put your final answer in <answer>...</answer> tags.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Parse and return the answer\n",
    "        answer = self._parse_answer(response_text)\n",
    "        return answer\n",
    "    \n",
    "    async def generate_solutions(self, problem: str, n_solutions: int = 2) -> list[TrajectoryView]:\n",
    "        \"\"\"Generate multiple solutions in parallel.\"\"\"\n",
    "        tasks = [\n",
    "            asyncio.create_task(self.generate_solution(problem))\n",
    "            for _ in range(n_solutions)\n",
    "        ]\n",
    "        # Returns list of TrajectoryView objects\n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "    def _parse_answer(self, response: str) -> str:\n",
    "        \"\"\"Extract answer from response.\"\"\"\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", response, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            return answer_match.group(1).strip()\n",
    "        else:\n",
    "            # Fallback: extract any number\n",
    "            return extract_answer(response)\n",
    "\n",
    "\n",
    "class MathJudge:\n",
    "    \"\"\"Judge that selects the best solution.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, model: str):\n",
    "        self.client = get_chat_client_async(base_url=base_url, api_key=\"EMPTY\")\n",
    "        self.model = model\n",
    "    \n",
    "    @trajectory(name=\"judge\")\n",
    "    async def judge_solutions(self, problem: str, solutions: list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Judge solutions using @trajectory decorator.\n",
    "        \n",
    "        Returns TrajectoryView with the selected solution in the result field.\n",
    "        \"\"\"\n",
    "        prompt = self._create_judge_prompt(problem, solutions)\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Parse and return the selected solution\n",
    "        return self._parse_judge_response(response_text, solutions)\n",
    "    \n",
    "    def _parse_judge_response(self, response: str, solutions: list[str]) -> str:\n",
    "        \"\"\"Parse judge response to get selected solution.\"\"\"\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", response, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            answer_text = answer_match.group(1).strip()\n",
    "            try:\n",
    "                solution_index = int(answer_text)\n",
    "                if 1 <= solution_index <= len(solutions):\n",
    "                    return solutions[solution_index - 1]\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "        # Fallback: return first solution\n",
    "        return solutions[0] if solutions else \"\"\n",
    "    \n",
    "    def _create_judge_prompt(self, problem: str, solutions: list[str]) -> str:\n",
    "        \"\"\"Create a prompt for the judge to evaluate solutions.\"\"\"\n",
    "        prompt = f\"\"\"You are an expert math verifier. Given a problem and multiple solution attempts, select the best solution.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Solutions to evaluate:\n",
    "\"\"\"\n",
    "        for i, solution in enumerate(solutions, 1):\n",
    "            prompt += f\"\\nSolution {i}: {solution}\\n\"\n",
    "        \n",
    "        prompt += \"\"\"\\nSelect the index of the best solution (1, 2, etc.) and output it in <answer>...</answer> tags.\n",
    "If multiple solutions are correct, output the index of the first correct one.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "print_section(\"Solver-Judge Classes Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Solver-Judge Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_section(\"Executing Solver-Judge Workflow\")\n\n# Create solver and judge\nsolver = MathSolver(base_url=proxy_url, model=MODEL)\njudge = MathJudge(base_url=proxy_url, model=MODEL)\n\n# Use first problem\nproblem = MATH_PROBLEMS[0]\ncolorful_print(f\"\\nProblem: {problem['question']}\", fg=\"yellow\", bold=True)\ncolorful_print(f\"Ground Truth: {problem['ground_truth']}\", fg=\"green\")\n\n# Generate 2 solutions\ncolorful_print(\"\\nGenerating 2 solutions...\", fg=\"cyan\")\nsolver_trajs = await solver.generate_solutions(problem[\"question\"], n_solutions=2)\n\nsolutions = []\nfor i, traj in enumerate(solver_trajs, 1):\n    solution = traj.result\n    solutions.append(solution)\n    reward = simple_math_reward(solution, problem[\"ground_truth\"])\n    traj.steps[0].reward = reward\n    \n    colorful_print(f\"\\nSolution {i}:\", fg=\"blue\", bold=True)\n    colorful_print(f\"  Answer: {solution}\", fg=\"white\")\n    reward_color = \"green\" if reward == 1.0 else \"red\"\n    colorful_print(f\"  Reward: {reward}\", fg=reward_color)\n    colorful_print(f\"  Trajectory Name: {traj.name}\", fg=\"blue\")\n    colorful_print(f\"  Steps: {len(traj.steps)}\", fg=\"blue\")\n\n# Judge solutions\ncolorful_print(\"\\nJudging solutions...\", fg=\"cyan\")\njudge_traj = await judge.judge_solutions(problem[\"question\"], solutions)\n\nselected_solution = judge_traj.result\njudge_reward = simple_math_reward(selected_solution, problem[\"ground_truth\"])\njudge_traj.steps[0].reward = judge_reward\njudge_traj.reward = judge_reward\n\ncolorful_print(\"\\nJudge Decision:\", fg=\"magenta\", bold=True)\ncolorful_print(f\"  Selected: {selected_solution}\", fg=\"white\")\nreward_color = \"green\" if judge_reward == 1.0 else \"red\"\ncolorful_print(f\"  Reward: {judge_reward}\", fg=reward_color, bold=True)\ncolorful_print(f\"  Trajectory Name: {judge_traj.name}\", fg=\"blue\")\n\ncolorful_print(\"\\n✓ Solver-Judge workflow completed!\", fg=\"green\", bold=True)\ncolorful_print(f\"Total trajectories: {len(solver_trajs) + 1} (2 solver + 1 judge)\", fg=\"cyan\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Solver-Judge Trajectories\n",
    "\n",
    "Let's create a custom visualization for the solver-judge workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def print_trajectory_view(traj: TrajectoryView, problem: dict):\n    \"\"\"\n    Print a TrajectoryView (from @trajectory decorator).\n    \"\"\"\n    colorful_print(\"\\n\" + \"=\" * 70, fg=\"cyan\", bold=True)\n    colorful_print(f\"TRAJECTORY: {traj.name}\", fg=\"cyan\", bold=True)\n    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n    \n    # Trajectory info\n    colorful_print(f\"Name: {traj.name}\", fg=\"blue\")\n    colorful_print(f\"Steps: {len(traj.steps)}\", fg=\"blue\")\n    \n    if traj.reward is not None:\n        reward_color = \"green\" if traj.reward == 1.0 else \"red\"\n        colorful_print(f\"Trajectory Reward: {traj.reward}\", fg=reward_color, bold=True)\n    \n    # Print each step\n    for i, step in enumerate(traj.steps, 1):\n        colorful_print(f\"\\n  Step {i}:\", fg=\"yellow\", bold=True)\n        colorful_print(f\"    Trace ID: {step.id}\", fg=\"blue\")\n        \n        if step.reward is not None:\n            reward_color = \"green\" if step.reward == 1.0 else \"red\"\n            colorful_print(f\"    Step Reward: {step.reward}\", fg=reward_color)\n    \n    # Result\n    colorful_print(f\"\\nResult: {traj.result}\", fg=\"white\", bold=True)\n    colorful_print(f\"Ground Truth: {problem['ground_truth']}\", fg=\"green\")\n    \n    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n\n\nprint_section(\"Visualizing Solver-Judge Trajectories\")\n\n# Print solver trajectories\nfor i, traj in enumerate(solver_trajs, 1):\n    colorful_print(f\"\\n--- Solver Trajectory {i} ---\", fg=\"blue\", bold=True)\n    print_trajectory_view(traj, problem)\n\n# Print judge trajectory\ncolorful_print(\"\\n--- Judge Trajectory ---\", fg=\"magenta\", bold=True)\nprint_trajectory_view(judge_traj, problem)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training with RLLM\n",
    "\n",
    "Now that we've collected trajectories, let's demonstrate how to use them for training.\n",
    "\n",
    "### 7.1 SFT Training (Supervised Fine-Tuning)\n",
    "\n",
    "For SFT, we use successful trajectories to fine-tune the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_section(\"SFT Training Preparation\")\n\ncolorful_print(\"\"\"\nFor SFT training, you would typically:\n1. Collect many trajectories (100s to 1000s)\n2. Filter for successful ones (reward > threshold)\n3. Format them as training data\n4. Use AgentSFTTrainer to fine-tune\n\nSince we only have a few trajectories, we'll skip actual SFT training here.\nFor a real example, see: examples/sft/train_math_sft.py\n\nKey steps:\n\"\"\", fg=\"yellow\")\n\ncolorful_print(\"\"\"\nfrom rllm.trainer.agent_sft_trainer import AgentSFTTrainer\n\n# Process trajectories (filter by reward, format as messages)\nsft_data = AgentSFTTrainer.process_trajectories(\n    trajectories=collected_trajectories,\n    reward_threshold=1.0,\n    filter_tool_calls=False,\n)\n\n# Save to file\nimport pandas as pd\npd.DataFrame(sft_data).to_parquet(\"sft_data.parquet\", index=False)\n\n# Create config and train\n@hydra.main(config_path=\"pkg://rllm.trainer.config\", config_name=\"sft_trainer\")\ndef main(config):\n    trainer = AgentSFTTrainer(config=config)\n    trainer.train()\n\"\"\", fg=\"green\")\n\ncolorful_print(\"\\n✓ SFT training overview complete\", fg=\"green\", bold=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 RL Training (Reinforcement Learning with PPO)\n",
    "\n",
    "For RL training, we use the rollout function directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_section(\"RL Training with AgentTrainer\")\n\ncolorful_print(\"\"\"\nNow let's actually run a minimal RL training demonstration!\n\nWe'll use:\n- Our simple math dataset\n- A rollout function that uses the proxy\n- Minimal training steps for demo purposes\n\"\"\", fg=\"yellow\")\n\n# Create datasets from our math problems\nfrom rllm.data.dataset import Dataset\n\ntrain_data = MATH_PROBLEMS * 2  # Repeat for more samples\ntrain_dataset = Dataset(data=train_data, name=\"tutorial_math\", split=\"train\")\n\ncolorful_print(f\"\\n✓ Created training dataset with {len(train_dataset)} samples\", fg=\"green\")\n\n# Define rollout function for training\ndef rollout_for_training(**kwargs):\n    \"\"\"\n    Rollout function for RL training.\n    \n    This function is called by the trainer for each sample.\n    It must recreate the client inside to avoid serialization issues with Ray.\n    \"\"\"\n    import asyncio\n    from rllm.sdk import get_chat_client_async, session\n    \n    question = kwargs[\"question\"]\n    ground_truth = kwargs[\"ground_truth\"]\n    \n    # Create client inside function (required for Ray serialization)\n    client = get_chat_client_async(base_url=proxy_url, api_key=\"EMPTY\")\n    \n    async def run():\n        with session(task=\"training\") as sess:\n            response = await client.chat.completions.create(\n                model=MODEL,\n                messages=[{\"role\": \"user\", \"content\": question}],\n                max_tokens=100,\n            )\n            response_text = response.choices[0].message.content\n            reward = simple_math_reward(response_text, ground_truth)\n            return reward\n    \n    # Run async function\n    return asyncio.run(run())\n\ncolorful_print(\"✓ Rollout function defined\", fg=\"green\")\n\n# Create minimal config for demonstration\nfrom omegaconf import OmegaConf\n\nminimal_config = OmegaConf.create({\n    \"trainer\": {\n        \"total_epochs\": 1,  # Just 1 epoch for demo\n        \"test_freq\": 1,\n    },\n    \"data\": {\n        \"train_batch_size\": 2,\n        \"val_batch_size\": 2,\n        \"max_prompt_length\": 512,\n        \"max_response_length\": 512,\n    },\n    \"actor_rollout_ref\": {\n        \"rollout_batch_size\": 2,\n        \"n_samples_per_prompt\": 1,\n    },\n    \"algorithm\": {\n        \"gamma\": 0.99,\n    },\n})\n\ncolorful_print(\"✓ Training config created\", fg=\"green\")\n\ncolorful_print(\"\"\"\n\\nNOTE: For this tutorial, we're using a minimal config with:\n- 1 epoch\n- Small batch sizes\n- Few samples\n\nIn production, you would use larger values and proper hyperparameters.\n\"\"\", fg=\"yellow\")\n\ncolorful_print(\"\"\"\nTo run actual training, you would do:\n\nfrom rllm.trainer.agent_trainer import AgentTrainer\n\ntrainer = AgentTrainer(\n    config=minimal_config,\n    train_dataset=train_dataset,\n    val_dataset=train_dataset,  # Using train as val for demo\n    agent_run_func=rollout_for_training,\n    backend=\"verl\",  # or \"tinker\"\n)\n\n# This would start the actual training\n# trainer.train()\n\nNote: Actual training requires proper GPU setup and can take time.\nFor this tutorial, we've defined everything but skipped the .train() call.\n\"\"\", fg=\"green\")\n\ncolorful_print(\"\\n✓ RL training setup complete\", fg=\"green\", bold=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Training with Solver-Judge Workflow\n",
    "\n",
    "You can also train with more complex workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_section(\"Workflow Training with Solver-Judge\")\n\ncolorful_print(\"\"\"\nYou can also train with more complex workflows like Solver-Judge.\n\nLet's set up a workflow-based training:\n\"\"\", fg=\"yellow\")\n\n# Define async workflow runner\nasync def run_solver_judge_workflow(**kwargs) -> float:\n    \"\"\"\n    Async workflow runner for Solver-Judge training.\n    \n    This combines multiple LLM calls into a single workflow.\n    \"\"\"\n    from rllm.sdk import get_chat_client_async\n    \n    question = kwargs[\"question\"]\n    ground_truth = kwargs[\"ground_truth\"]\n    \n    # Create solver and judge (must be inside function for Ray)\n    solver = MathSolver(base_url=proxy_url, model=MODEL)\n    judge = MathJudge(base_url=proxy_url, model=MODEL)\n    \n    # Generate solutions\n    solver_trajs = await solver.generate_solutions(question, n_solutions=2)\n    solutions = [traj.result for traj in solver_trajs]\n    \n    # Judge solutions\n    judge_traj = await judge.judge_solutions(question, solutions)\n    selected = judge_traj.result\n    \n    # Calculate reward\n    reward = simple_math_reward(selected, ground_truth)\n    \n    return reward\n\ncolorful_print(\"✓ Solver-Judge workflow function defined\", fg=\"green\")\n\ncolorful_print(\"\"\"\nTo run workflow training:\n\nfrom rllm.trainer.agent_trainer import AgentTrainer\n\ntrainer = AgentTrainer(\n    config=minimal_config,\n    train_dataset=train_dataset,\n    val_dataset=train_dataset,\n    agent_run_func=run_solver_judge_workflow,  # Use workflow function\n    backend=\"verl\",\n)\n\n# This would start training with the Solver-Judge workflow\n# trainer.train()\n\nBenefits of workflow training:\n- Multiple LLM calls per sample (solver + judge)\n- Automatic trace collection for all steps\n- Richer learning signal from multi-step reasoning\n- Can optimize entire workflow end-to-end\n\"\"\", fg=\"green\")\n\ncolorful_print(\"\\n✓ Workflow training setup complete\", fg=\"green\", bold=True)\n\ncolorful_print(\"\"\"\n\\n================================================\nTRAINING SUMMARY\n================================================\n\nWe've demonstrated 3 training approaches:\n\n1. SFT Training (Supervised Fine-Tuning)\n   - Use successful trajectories to fine-tune\n   - Good for bootstrapping from demonstrations\n   - See: examples/sft/train_math_sft.py\n\n2. RL Training (Reinforcement Learning)\n   - Use AgentTrainer with simple rollout function\n   - Learns from reward feedback\n   - Optimizes policy with PPO\n\n3. Workflow Training\n   - Multi-step LLM workflows (e.g., Solver-Judge)\n   - More complex reasoning patterns\n   - End-to-end optimization\n\nAll approaches integrate seamlessly with:\n- ProxyManager for inference\n- SQLite trace storage\n- Automatic session tracking\n- Reward-based learning\n\n\"\"\", fg=\"cyan\", bold=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Cleanup\")\n",
    "\n",
    "# Shutdown proxy\n",
    "proxy_manager.shutdown_proxy()\n",
    "colorful_print(\"✓ Proxy shutdown complete\", fg=\"green\")\n",
    "\n",
    "colorful_print(\"\\n\" + \"=\" * 70, fg=\"cyan\", bold=True)\n",
    "colorful_print(\"TUTORIAL COMPLETE!\", fg=\"green\", bold=True)\n",
    "colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "\n",
    "colorful_print(\"\"\"\n",
    "Summary:\n",
    "✓ Started LiteLLM proxy with OpenAI\n",
    "✓ Executed simple rollouts\n",
    "✓ Retrieved traces from SQLite\n",
    "✓ Visualized trajectories with color\n",
    "✓ Demonstrated Solver-Judge pattern with @trajectory decorator\n",
    "✓ Showed how to prepare for SFT training\n",
    "✓ Showed how to prepare for RL training\n",
    "\n",
    "Next Steps:\n",
    "- Collect more trajectories (100s to 1000s)\n",
    "- Set up training environment with GPUs\n",
    "- Configure Hydra configs for your use case\n",
    "- Run actual training with AgentSFTTrainer or AgentTrainer\n",
    "- Monitor training with wandb/tensorboard\n",
    "\"\"\", fg=\"yellow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}