{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLLM SDK Tutorial: From Rollouts to Training\n",
    "\n",
    "This tutorial demonstrates the complete RLLM SDK workflow:\n",
    "\n",
    "1. **Setup**: Start proxy manager and configure OpenAI\n",
    "2. **Simple Rollout**: Execute a basic rollout function\n",
    "3. **Trace Retrieval**: Fetch traces from SQLite storage\n",
    "4. **Trajectory Visualization**: Print and visualize trajectories\n",
    "5. **Solver-Judge Pattern**: Advanced workflow with decorators\n",
    "6. **SFT Training**: Supervised fine-tuning on collected traces\n",
    "7. **RL Training**: Reinforcement learning with PPO\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "- OpenAI API key set: `export OPENAI_API_KEY=\"sk-...\"`\n",
    "- RLLM installed: `pip install -e .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import click\n",
    "\n",
    "from rllm.sdk import (\n",
    "    get_chat_client_async,\n",
    "    session,\n",
    "    trajectory,\n",
    "    TrajectoryView,\n",
    ")\n",
    "from rllm.sdk.proxy.proxy_manager import ProxyManager\n",
    "from rllm.sdk.store import SqliteTraceStore\n",
    "\n",
    "# Check for OpenAI API key\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"Please set OPENAI_API_KEY environment variable\")\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Printing Utility\n",
    "\n",
    "We'll use this helper function for colorful output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorful_print(string: str, *args, **kwargs) -> None:\n",
    "    \"\"\"Print colored text using click.\"\"\"\n",
    "    end = kwargs.pop(\"end\", \"\\n\")\n",
    "    print(click.style(string, *args, **kwargs), end=end, flush=True)\n",
    "\n",
    "\n",
    "def print_section(title: str):\n",
    "    \"\"\"Print a section header.\"\"\"\n",
    "    colorful_print(\"\\n\" + \"=\" * 70, fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\" {title}\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "\n",
    "\n",
    "print_section(\"Color Printing Utility Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Proxy Manager\n",
    "\n",
    "The ProxyManager automatically starts a LiteLLM proxy server that handles:\n",
    "- OpenAI API calls\n",
    "- Automatic trace collection\n",
    "- Persistent storage in SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DB_PATH = \"/tmp/rllm_tutorial.db\"\n",
    "PROXY_PORT = 4000\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Clean up existing database\n",
    "db_file = Path(DB_PATH)\n",
    "if db_file.exists():\n",
    "    db_file.unlink()\n",
    "    print(f\"✓ Removed existing database: {DB_PATH}\")\n",
    "\n",
    "# Create OpenAI configuration\n",
    "config = {\n",
    "    \"model_list\": [\n",
    "        {\n",
    "            \"model_name\": MODEL,\n",
    "            \"litellm_params\": {\n",
    "                \"model\": MODEL,\n",
    "                \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create and start proxy manager\n",
    "proxy_manager = ProxyManager(\n",
    "    proxy_host=\"127.0.0.1\",\n",
    "    proxy_port=PROXY_PORT,\n",
    "    admin_token=\"tutorial-admin-token\",\n",
    ")\n",
    "\n",
    "# Start proxy subprocess\n",
    "config_path = proxy_manager.start_proxy_subprocess(\n",
    "    config=config,\n",
    "    db_path=DB_PATH,\n",
    "    project=\"tutorial-project\",\n",
    ")\n",
    "\n",
    "proxy_url = proxy_manager.get_proxy_url(include_v1=True)\n",
    "print_section(\"Proxy Manager Started\")\n",
    "colorful_print(f\"Proxy URL: {proxy_url}\", fg=\"green\")\n",
    "colorful_print(f\"Database: {DB_PATH}\", fg=\"green\")\n",
    "colorful_print(f\"Model: {MODEL}\", fg=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Dataset\n",
    "\n",
    "We'll create a tiny math dataset for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple math problems\n",
    "MATH_PROBLEMS = [\n",
    "    {\n",
    "        \"question\": \"What is 15 + 27?\",\n",
    "        \"ground_truth\": \"42\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If a rectangle has length 8 and width 5, what is its area?\",\n",
    "        \"ground_truth\": \"40\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the square root of 144?\",\n",
    "        \"ground_truth\": \"12\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print_section(\"Dataset Created\")\n",
    "for i, problem in enumerate(MATH_PROBLEMS, 1):\n",
    "    colorful_print(f\"Problem {i}: {problem['question']}\", fg=\"yellow\")\n",
    "    colorful_print(f\"  Answer: {problem['ground_truth']}\", fg=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Extract numerical answer from text.\"\"\"\n",
    "    # Look for numbers in the text\n",
    "    numbers = re.findall(r'\\b\\d+\\.?\\d*\\b', text)\n",
    "    return numbers[-1] if numbers else \"\"\n",
    "\n",
    "\n",
    "def simple_math_reward(response: str, ground_truth: str) -> float:\n",
    "    \"\"\"Simple exact match reward.\"\"\"\n",
    "    predicted = extract_answer(response)\n",
    "    return 1.0 if predicted == ground_truth else 0.0\n",
    "\n",
    "\n",
    "# Test the reward function\n",
    "test_response = \"The answer is 42\"\n",
    "test_gt = \"42\"\n",
    "reward = simple_math_reward(test_response, test_gt)\n",
    "colorful_print(f\"\\nTest reward: {reward}\", fg=\"green\" if reward == 1.0 else \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Rollout Function\n",
    "\n",
    "This is the simplest version - similar to `train_hendrycks_math.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def simple_rollout(question: str, ground_truth: str) -> tuple[str, float, str]:\n",
    "    \"\"\"\n",
    "    Execute a simple rollout.\n",
    "    \n",
    "    Args:\n",
    "        question: The math problem\n",
    "        ground_truth: The correct answer\n",
    "    \n",
    "    Returns:\n",
    "        (session_uid, reward, response_text)\n",
    "    \"\"\"\n",
    "    client = get_chat_client_async(base_url=proxy_url, api_key=\"EMPTY\")\n",
    "    \n",
    "    with session(task=\"simple_math\") as sess:\n",
    "        session_uid = sess._uid\n",
    "        \n",
    "        # Make LLM call\n",
    "        response = await client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = simple_math_reward(response_text, ground_truth)\n",
    "        \n",
    "        return session_uid, reward, response_text\n",
    "\n",
    "\n",
    "print_section(\"Simple Rollout Function Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Executing Rollouts\")\n",
    "\n",
    "rollout_results = []\n",
    "\n",
    "for i, problem in enumerate(MATH_PROBLEMS, 1):\n",
    "    colorful_print(f\"\\nRollout {i}/{len(MATH_PROBLEMS)}\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\"Question: {problem['question']}\", fg=\"yellow\")\n",
    "    \n",
    "    session_uid, reward, response = await simple_rollout(\n",
    "        problem[\"question\"],\n",
    "        problem[\"ground_truth\"]\n",
    "    )\n",
    "    \n",
    "    rollout_results.append({\n",
    "        \"session_uid\": session_uid,\n",
    "        \"question\": problem[\"question\"],\n",
    "        \"ground_truth\": problem[\"ground_truth\"],\n",
    "        \"response\": response,\n",
    "        \"reward\": reward,\n",
    "    })\n",
    "    \n",
    "    colorful_print(f\"Response: {response}\", fg=\"white\")\n",
    "    reward_color = \"green\" if reward == 1.0 else \"red\"\n",
    "    colorful_print(f\"Reward: {reward}\", fg=reward_color, bold=True)\n",
    "    colorful_print(f\"Session UID: {session_uid}\", fg=\"blue\")\n",
    "\n",
    "avg_reward = sum(r[\"reward\"] for r in rollout_results) / len(rollout_results)\n",
    "colorful_print(f\"\\n✓ Average Reward: {avg_reward:.2f}\", fg=\"green\", bold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetch Traces from SQLite Store\n",
    "\n",
    "After executing rollouts, we flush the tracer and retrieve traces from SQLite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Flushing and Retrieving Traces\")\n",
    "\n",
    "# Flush tracer to ensure all traces are written to database\n",
    "flush_result = await proxy_manager.flush_tracer(timeout=30.0)\n",
    "colorful_print(f\"✓ Flush result: {flush_result}\", fg=\"green\")\n",
    "\n",
    "# Create store and retrieve traces\n",
    "store = SqliteTraceStore(db_path=DB_PATH)\n",
    "\n",
    "all_traces = []\n",
    "for result in rollout_results:\n",
    "    session_uid = result[\"session_uid\"]\n",
    "    traces = await store.get_by_session_uid(session_uid)\n",
    "    \n",
    "    colorful_print(f\"\\nSession {session_uid}:\", fg=\"cyan\")\n",
    "    colorful_print(f\"  Found {len(traces)} trace(s)\", fg=\"yellow\")\n",
    "    \n",
    "    for trace in traces:\n",
    "        colorful_print(f\"    Trace ID: {trace.id}\", fg=\"blue\")\n",
    "        all_traces.append((trace, result))\n",
    "\n",
    "colorful_print(f\"\\n✓ Total traces retrieved: {len(all_traces)}\", fg=\"green\", bold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trajectory Visualization\n",
    "\n",
    "Let's create a nice visualization function for trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trajectory(trace, metadata: dict):\n",
    "    \"\"\"\n",
    "    Print a trajectory with color-coded information.\n",
    "    \n",
    "    Args:\n",
    "        trace: Trace object from SQLite store\n",
    "        metadata: Additional metadata (question, reward, etc.)\n",
    "    \"\"\"\n",
    "    data = trace.data\n",
    "    \n",
    "    colorful_print(\"\\n\" + \"=\" * 70, fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\"TRAJECTORY: {trace.id}\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "    \n",
    "    # Session info\n",
    "    colorful_print(f\"Session UID: {metadata['session_uid']}\", fg=\"blue\")\n",
    "    \n",
    "    # Model info\n",
    "    colorful_print(f\"Model: {data['model']}\", fg=\"white\")\n",
    "    colorful_print(f\"Latency: {data['latency_ms']:.2f} ms\", fg=\"white\")\n",
    "    \n",
    "    # Token usage\n",
    "    tokens = data[\"tokens\"]\n",
    "    colorful_print(\n",
    "        f\"Tokens: prompt={tokens['prompt']}, completion={tokens['completion']}, total={tokens['total']}\",\n",
    "        fg=\"white\"\n",
    "    )\n",
    "    \n",
    "    # Input\n",
    "    colorful_print(\"\\nINPUT:\", fg=\"yellow\", bold=True)\n",
    "    input_messages = data[\"input\"][\"messages\"]\n",
    "    for msg in input_messages:\n",
    "        role_color = \"green\" if msg[\"role\"] == \"user\" else \"blue\"\n",
    "        colorful_print(f\"  [{msg['role']}]: {msg['content']}\", fg=role_color)\n",
    "    \n",
    "    # Output\n",
    "    colorful_print(\"\\nOUTPUT:\", fg=\"magenta\", bold=True)\n",
    "    output_choices = data[\"output\"][\"choices\"]\n",
    "    output_text = output_choices[0][\"message\"][\"content\"]\n",
    "    colorful_print(f\"  {output_text}\", fg=\"white\")\n",
    "    \n",
    "    # Ground truth and reward\n",
    "    colorful_print(\"\\nEVALUATION:\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\"  Ground Truth: {metadata['ground_truth']}\", fg=\"green\")\n",
    "    \n",
    "    reward = metadata[\"reward\"]\n",
    "    reward_color = \"green\" if reward == 1.0 else \"red\"\n",
    "    reward_symbol = \"✓\" if reward == 1.0 else \"✗\"\n",
    "    colorful_print(f\"  Reward: {reward_symbol} {reward}\", fg=reward_color, bold=True)\n",
    "    \n",
    "    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "\n",
    "\n",
    "print_section(\"Trajectory Visualization Function Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize All Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Visualizing Trajectories\")\n",
    "\n",
    "for trace, metadata in all_traces:\n",
    "    print_trajectory(trace, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Solver-Judge Pattern with Decorators\n",
    "\n",
    "Now let's demonstrate a more complex workflow using the `@trajectory` decorator.\n",
    "This shows how RLLM SDK automatically formats trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathSolver:\n",
    "    \"\"\"Solver that generates multiple solutions.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, model: str):\n",
    "        self.client = get_chat_client_async(base_url=base_url, api_key=\"EMPTY\")\n",
    "        self.model = model\n",
    "    \n",
    "    @trajectory(name=\"solver\")\n",
    "    async def generate_solution(self, problem: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a solution using @trajectory decorator.\n",
    "        \n",
    "        The decorator:\n",
    "        - Creates a session internally\n",
    "        - Tracks LLM calls automatically\n",
    "        - Returns TrajectoryView with result field set to the return value\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{problem}\\n\\nPlease solve this problem and put your final answer in <answer>...</answer> tags.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Parse and return the answer\n",
    "        answer = self._parse_answer(response_text)\n",
    "        return answer\n",
    "    \n",
    "    async def generate_solutions(self, problem: str, n_solutions: int = 2) -> list[TrajectoryView]:\n",
    "        \"\"\"Generate multiple solutions in parallel.\"\"\"\n",
    "        tasks = [\n",
    "            asyncio.create_task(self.generate_solution(problem))\n",
    "            for _ in range(n_solutions)\n",
    "        ]\n",
    "        # Returns list of TrajectoryView objects\n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "    def _parse_answer(self, response: str) -> str:\n",
    "        \"\"\"Extract answer from response.\"\"\"\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", response, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            return answer_match.group(1).strip()\n",
    "        else:\n",
    "            # Fallback: extract any number\n",
    "            return extract_answer(response)\n",
    "\n",
    "\n",
    "class MathJudge:\n",
    "    \"\"\"Judge that selects the best solution.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, model: str):\n",
    "        self.client = get_chat_client_async(base_url=base_url, api_key=\"EMPTY\")\n",
    "        self.model = model\n",
    "    \n",
    "    @trajectory(name=\"judge\")\n",
    "    async def judge_solutions(self, problem: str, solutions: list[str]) -> str:\n",
    "        \"\"\"\n",
    "        Judge solutions using @trajectory decorator.\n",
    "        \n",
    "        Returns TrajectoryView with the selected solution in the result field.\n",
    "        \"\"\"\n",
    "        prompt = self._create_judge_prompt(problem, solutions)\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Parse and return the selected solution\n",
    "        return self._parse_judge_response(response_text, solutions)\n",
    "    \n",
    "    def _parse_judge_response(self, response: str, solutions: list[str]) -> str:\n",
    "        \"\"\"Parse judge response to get selected solution.\"\"\"\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", response, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            answer_text = answer_match.group(1).strip()\n",
    "            try:\n",
    "                solution_index = int(answer_text)\n",
    "                if 1 <= solution_index <= len(solutions):\n",
    "                    return solutions[solution_index - 1]\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "        # Fallback: return first solution\n",
    "        return solutions[0] if solutions else \"\"\n",
    "    \n",
    "    def _create_judge_prompt(self, problem: str, solutions: list[str]) -> str:\n",
    "        \"\"\"Create a prompt for the judge to evaluate solutions.\"\"\"\n",
    "        prompt = f\"\"\"You are an expert math verifier. Given a problem and multiple solution attempts, select the best solution.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Solutions to evaluate:\n",
    "\"\"\"\n",
    "        for i, solution in enumerate(solutions, 1):\n",
    "            prompt += f\"\\nSolution {i}: {solution}\\n\"\n",
    "        \n",
    "        prompt += \"\"\"\\nSelect the index of the best solution (1, 2, etc.) and output it in <answer>...</answer> tags.\n",
    "If multiple solutions are correct, output the index of the first correct one.\"\"\"\n",
    "        return prompt\n",
    "\n",
    "\n",
    "print_section(\"Solver-Judge Classes Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Solver-Judge Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Executing Solver-Judge Workflow\")\n",
    "\n",
    "# Create solver and judge\n",
    "solver = MathSolver(base_url=proxy_url, model=MODEL)\n",
    "judge = MathJudge(base_url=proxy_url, model=MODEL)\n",
    "\n",
    "# Use first problem\n",
    "problem = MATH_PROBLEMS[0]\n",
    "colorful_print(f\"\\nProblem: {problem['question']}\", fg=\"yellow\", bold=True)\n",
    "colorful_print(f\"Ground Truth: {problem['ground_truth']}\", fg=\"green\")\n",
    "\n",
    "# Generate 2 solutions\n",
    "colorful_print(\"\\nGenerating 2 solutions...\", fg=\"cyan\")\n",
    "solver_trajs = await solver.generate_solutions(problem[\"question\"], n_solutions=2)\n",
    "\n",
    "solutions = []\n",
    "for i, traj in enumerate(solver_trajs, 1):\n",
    "    solution = traj.result\n",
    "    solutions.append(solution)\n",
    "    reward = simple_math_reward(solution, problem[\"ground_truth\"])\n",
    "    traj.steps[0].reward = reward\n",
    "    \n",
    "    colorful_print(f\"\\nSolution {i}:\", fg=\"blue\", bold=True)\n",
    "    colorful_print(f\"  Answer: {solution}\", fg=\"white\")\n",
    "    reward_color = \"green\" if reward == 1.0 else \"red\"\n",
    "    colorful_print(f\"  Reward: {reward}\", fg=reward_color)\n",
    "    colorful_print(f\"  Trajectory ID: {traj.trajectory_id}\", fg=\"blue\")\n",
    "    colorful_print(f\"  Steps: {len(traj.steps)}\", fg=\"blue\")\n",
    "\n",
    "# Judge solutions\n",
    "colorful_print(\"\\nJudging solutions...\", fg=\"cyan\")\n",
    "judge_traj = await judge.judge_solutions(problem[\"question\"], solutions)\n",
    "\n",
    "selected_solution = judge_traj.result\n",
    "judge_reward = simple_math_reward(selected_solution, problem[\"ground_truth\"])\n",
    "judge_traj.steps[0].reward = judge_reward\n",
    "judge_traj.reward = judge_reward\n",
    "\n",
    "colorful_print(\"\\nJudge Decision:\", fg=\"magenta\", bold=True)\n",
    "colorful_print(f\"  Selected: {selected_solution}\", fg=\"white\")\n",
    "reward_color = \"green\" if judge_reward == 1.0 else \"red\"\n",
    "colorful_print(f\"  Reward: {judge_reward}\", fg=reward_color, bold=True)\n",
    "colorful_print(f\"  Trajectory ID: {judge_traj.trajectory_id}\", fg=\"blue\")\n",
    "\n",
    "colorful_print(\"\\n✓ Solver-Judge workflow completed!\", fg=\"green\", bold=True)\n",
    "colorful_print(f\"Total trajectories: {len(solver_trajs) + 1} (2 solver + 1 judge)\", fg=\"cyan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Solver-Judge Trajectories\n",
    "\n",
    "Let's create a custom visualization for the solver-judge workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trajectory_view(traj: TrajectoryView, problem: dict):\n",
    "    \"\"\"\n",
    "    Print a TrajectoryView (from @trajectory decorator).\n",
    "    \"\"\"\n",
    "    colorful_print(\"\\n\" + \"=\" * 70, fg=\"cyan\", bold=True)\n",
    "    colorful_print(f\"TRAJECTORY: {traj.name} - {traj.trajectory_id}\", fg=\"cyan\", bold=True)\n",
    "    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "    \n",
    "    # Trajectory info\n",
    "    colorful_print(f\"Name: {traj.name}\", fg=\"blue\")\n",
    "    colorful_print(f\"Steps: {len(traj.steps)}\", fg=\"blue\")\n",
    "    \n",
    "    if traj.reward is not None:\n",
    "        reward_color = \"green\" if traj.reward == 1.0 else \"red\"\n",
    "        colorful_print(f\"Trajectory Reward: {traj.reward}\", fg=reward_color, bold=True)\n",
    "    \n",
    "    # Print each step\n",
    "    for i, step in enumerate(traj.steps, 1):\n",
    "        colorful_print(f\"\\n  Step {i}:\", fg=\"yellow\", bold=True)\n",
    "        colorful_print(f\"    Trace ID: {step.trace_id}\", fg=\"blue\")\n",
    "        \n",
    "        if step.reward is not None:\n",
    "            reward_color = \"green\" if step.reward == 1.0 else \"red\"\n",
    "            colorful_print(f\"    Step Reward: {step.reward}\", fg=reward_color)\n",
    "    \n",
    "    # Result\n",
    "    colorful_print(f\"\\nResult: {traj.result}\", fg=\"white\", bold=True)\n",
    "    colorful_print(f\"Ground Truth: {problem['ground_truth']}\", fg=\"green\")\n",
    "    \n",
    "    colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "\n",
    "\n",
    "print_section(\"Visualizing Solver-Judge Trajectories\")\n",
    "\n",
    "# Print solver trajectories\n",
    "for i, traj in enumerate(solver_trajs, 1):\n",
    "    colorful_print(f\"\\n--- Solver Trajectory {i} ---\", fg=\"blue\", bold=True)\n",
    "    print_trajectory_view(traj, problem)\n",
    "\n",
    "# Print judge trajectory\n",
    "colorful_print(\"\\n--- Judge Trajectory ---\", fg=\"magenta\", bold=True)\n",
    "print_trajectory_view(judge_traj, problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training with RLLM\n",
    "\n",
    "Now that we've collected trajectories, let's demonstrate how to use them for training.\n",
    "\n",
    "### 7.1 SFT Training (Supervised Fine-Tuning)\n",
    "\n",
    "For SFT, we use successful trajectories to fine-tune the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"SFT Training Preparation\")\n",
    "\n",
    "# In a real scenario, you would:\n",
    "# 1. Collect many trajectories (100s to 1000s)\n",
    "# 2. Filter for successful ones (reward > threshold)\n",
    "# 3. Format them as SFT training data\n",
    "# 4. Use AgentSFTTrainer to fine-tune\n",
    "\n",
    "colorful_print(\"\"\"\n",
    "SFT Training Steps:\n",
    "\n",
    "1. Collect trajectories using rollouts\n",
    "2. Filter by reward threshold (e.g., reward >= 1.0)\n",
    "3. Convert to training format\n",
    "4. Train with AgentSFTTrainer\n",
    "\n",
    "Example code:\n",
    "\"\"\", fg=\"yellow\")\n",
    "\n",
    "colorful_print(\"\"\"\n",
    "from rllm.trainer.agent_sft_trainer import AgentSFTTrainer\n",
    "\n",
    "# Process trajectories (filter by reward, format as messages)\n",
    "sft_data = AgentSFTTrainer.process_trajectories(\n",
    "    trajectories=collected_trajectories,\n",
    "    reward_threshold=1.0,\n",
    "    filter_tool_calls=False,  # Set to True if using tool calls\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "import pandas as pd\n",
    "pd.DataFrame(sft_data).to_parquet(\"sft_data.parquet\", index=False)\n",
    "\n",
    "# Train with hydra config\n",
    "# trainer = AgentSFTTrainer(config=config)\n",
    "# trainer.train()\n",
    "\"\"\", fg=\"green\")\n",
    "\n",
    "colorful_print(\"\\n✓ SFT training overview complete\", fg=\"green\", bold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 RL Training (Reinforcement Learning with PPO)\n",
    "\n",
    "For RL training, we use the rollout function directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"RL Training Preparation\")\n",
    "\n",
    "# Define a rollout function for RL training\n",
    "# This is what gets passed to AgentTrainer\n",
    "\n",
    "colorful_print(\"\"\"\n",
    "RL Training Steps:\n",
    "\n",
    "1. Define a rollout function that:\n",
    "   - Takes task kwargs (question, ground_truth, etc.)\n",
    "   - Executes LLM calls\n",
    "   - Returns reward\n",
    "\n",
    "2. Create AgentTrainer with:\n",
    "   - Rollout function\n",
    "   - Training/validation datasets\n",
    "   - PPO config\n",
    "\n",
    "3. Train!\n",
    "\n",
    "Example code:\n",
    "\"\"\", fg=\"yellow\")\n",
    "\n",
    "colorful_print(\"\"\"\n",
    "from rllm.trainer.agent_trainer import AgentTrainer\n",
    "from rllm.data.dataset import DatasetRegistry\n",
    "import hydra\n",
    "\n",
    "# Define rollout function\n",
    "def rollout(**kwargs):\n",
    "    question = kwargs[\"question\"]\n",
    "    ground_truth = kwargs[\"ground_truth\"]\n",
    "    \n",
    "    # Create client (must be inside function for Ray serialization)\n",
    "    client = get_chat_client(base_url=\"http://localhost:4000/v1\", api_key=\"EMPTY\")\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    )\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    reward = simple_math_reward(response_text, ground_truth)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = DatasetRegistry.load_dataset(\"math\", \"train\")\n",
    "val_dataset = DatasetRegistry.load_dataset(\"math\", \"test\")\n",
    "\n",
    "# Create trainer\n",
    "@hydra.main(config_path=\"pkg://rllm.trainer.config\", config_name=\"agent_ppo_trainer\", version_base=None)\n",
    "def main(config):\n",
    "    trainer = AgentTrainer(\n",
    "        config=config,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        agent_run_func=rollout,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\", fg=\"green\")\n",
    "\n",
    "colorful_print(\"\\n✓ RL training overview complete\", fg=\"green\", bold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Training with Solver-Judge Workflow\n",
    "\n",
    "You can also train with more complex workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Workflow Training\")\n",
    "\n",
    "colorful_print(\"\"\"\n",
    "For complex workflows (like Solver-Judge), you can use the workflow directly:\n",
    "\n",
    "Example code:\n",
    "\"\"\", fg=\"yellow\")\n",
    "\n",
    "colorful_print(\"\"\"\n",
    "from rllm.trainer.agent_trainer import AgentTrainer\n",
    "\n",
    "# Define async workflow runner\n",
    "async def run_solver_judge_workflow(**kwargs) -> float:\n",
    "    question = kwargs[\"question\"]\n",
    "    ground_truth = kwargs[\"ground_truth\"]\n",
    "    \n",
    "    # Create solver and judge\n",
    "    solver = MathSolver(base_url=\"http://localhost:4000/v1\", model=\"gpt-4o-mini\")\n",
    "    judge = MathJudge(base_url=\"http://localhost:4000/v1\", model=\"gpt-4o-mini\")\n",
    "    \n",
    "    # Generate solutions\n",
    "    solver_trajs = await solver.generate_solutions(question, n_solutions=2)\n",
    "    solutions = [traj.result for traj in solver_trajs]\n",
    "    \n",
    "    # Judge solutions\n",
    "    judge_traj = await judge.judge_solutions(question, solutions)\n",
    "    selected = judge_traj.result\n",
    "    \n",
    "    # Calculate reward\n",
    "    reward = simple_math_reward(selected, ground_truth)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Create trainer with workflow\n",
    "@hydra.main(config_path=\"pkg://rllm.trainer.config\", config_name=\"agent_ppo_trainer\", version_base=None)\n",
    "def main(config):\n",
    "    trainer = AgentTrainer(\n",
    "        config=config,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        agent_run_func=run_solver_judge_workflow,\n",
    "    )\n",
    "    trainer.train()\n",
    "\"\"\", fg=\"green\")\n",
    "\n",
    "colorful_print(\"\\n✓ Workflow training overview complete\", fg=\"green\", bold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Cleanup\")\n",
    "\n",
    "# Shutdown proxy\n",
    "proxy_manager.shutdown_proxy()\n",
    "colorful_print(\"✓ Proxy shutdown complete\", fg=\"green\")\n",
    "\n",
    "colorful_print(\"\\n\" + \"=\" * 70, fg=\"cyan\", bold=True)\n",
    "colorful_print(\"TUTORIAL COMPLETE!\", fg=\"green\", bold=True)\n",
    "colorful_print(\"=\" * 70, fg=\"cyan\", bold=True)\n",
    "\n",
    "colorful_print(\"\"\"\n",
    "Summary:\n",
    "✓ Started LiteLLM proxy with OpenAI\n",
    "✓ Executed simple rollouts\n",
    "✓ Retrieved traces from SQLite\n",
    "✓ Visualized trajectories with color\n",
    "✓ Demonstrated Solver-Judge pattern with @trajectory decorator\n",
    "✓ Showed how to prepare for SFT training\n",
    "✓ Showed how to prepare for RL training\n",
    "\n",
    "Next Steps:\n",
    "- Collect more trajectories (100s to 1000s)\n",
    "- Set up training environment with GPUs\n",
    "- Configure Hydra configs for your use case\n",
    "- Run actual training with AgentSFTTrainer or AgentTrainer\n",
    "- Monitor training with wandb/tensorboard\n",
    "\"\"\", fg=\"yellow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
