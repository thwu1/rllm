{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLLM SDK Quick Start: Make Any Agent Trainable\n",
    "\n",
    "This tutorial shows how to make **any existing agent code** trainable with minimal changes.\n",
    "\n",
    "**The key insight:** Just replace your OpenAI client with the SDK client, and everything is automatically tracked for training!\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install -e .\n",
    "pip install litellm openai\n",
    "export OPENAI_API_KEY=\"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Start the Proxy\n",
    "\n",
    "One-time setup - start the proxy manager that tracks all LLM calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from rllm.sdk.proxy.proxy_manager import ProxyManager\n",
    "\n",
    "# Setup\n",
    "DB_PATH = \"/tmp/rllm_demo.db\"\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Clean up\n",
    "Path(DB_PATH).unlink(missing_ok=True)\n",
    "\n",
    "# Start proxy\n",
    "proxy_manager = ProxyManager(proxy_port=4000)\n",
    "config = {\n",
    "    \"model_list\": [{\n",
    "        \"model_name\": MODEL,\n",
    "        \"litellm_params\": {\n",
    "            \"model\": MODEL,\n",
    "            \"api_key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "        }\n",
    "    }]\n",
    "}\n",
    "proxy_manager.start_proxy_subprocess(config=config, db_path=DB_PATH, project=\"demo\")\n",
    "proxy_url = proxy_manager.get_proxy_url(include_v1=True)\n",
    "\n",
    "print(f\"âœ“ Proxy started at {proxy_url}\")\n",
    "print(f\"âœ“ Database: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Your Original Agent Code\n",
    "\n",
    "Here's a typical agent using the standard OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "class MathSolver:\n",
    "    \"\"\"A simple math solving agent - ORIGINAL VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        # Standard OpenAI client\n",
    "        self.client = AsyncOpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "    \n",
    "    async def solve(self, problem: str) -> str:\n",
    "        \"\"\"Solve a math problem.\"\"\"\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Solve this problem: {problem}\"}\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Use it\n",
    "original_solver = MathSolver(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "result = await original_solver.solve(\"What is 25 + 17?\")\n",
    "\n",
    "print(\"Original Agent Result:\")\n",
    "print(result)\n",
    "print(\"\\nâŒ This call is NOT tracked - can't use for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make It Trainable (2 Simple Changes!)\n",
    "\n",
    "**Change 1:** Import the SDK client instead of OpenAI client  \n",
    "**Change 2:** Point to the proxy URL\n",
    "\n",
    "That's it! Everything else stays the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllm.sdk import get_chat_client_async, session  # Change 1: Import SDK client\n",
    "\n",
    "class TrainableMathSolver:\n",
    "    \"\"\"A simple math solving agent - SDK VERSION (trainable!)\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, model: str = \"gpt-4o-mini\"):\n",
    "        # Change 2: Use SDK client pointing to proxy\n",
    "        self.client = get_chat_client_async(base_url=base_url, api_key=\"EMPTY\")\n",
    "        self.model = model\n",
    "    \n",
    "    async def solve(self, problem: str) -> str:\n",
    "        \"\"\"Solve a math problem - now tracked!\"\"\"\n",
    "        # Wrap in session context to track this rollout\n",
    "        with session(task=\"math_solving\") as sess:\n",
    "            response = await self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Solve this problem: {problem}\"}\n",
    "                ],\n",
    "                max_tokens=200,\n",
    "            )\n",
    "            # Store session UID for later retrieval\n",
    "            self.last_session_uid = sess._uid\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "# Use it - exact same interface!\n",
    "trainable_solver = TrainableMathSolver(base_url=proxy_url)\n",
    "result = await trainable_solver.solve(\"What is 25 + 17?\")\n",
    "\n",
    "print(\"SDK Agent Result:\")\n",
    "print(result)\n",
    "print(f\"\\nâœ… This call IS tracked! Session: {trainable_solver.last_session_uid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve the Traces\n",
    "\n",
    "Now you can retrieve **everything** about that interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllm.sdk.store import SqliteTraceStore\n",
    "\n",
    "# Flush to ensure traces are written\n",
    "await proxy_manager.flush_tracer(timeout=10.0)\n",
    "\n",
    "# Retrieve traces by session UID\n",
    "store = SqliteTraceStore(db_path=DB_PATH)\n",
    "traces = await store.get_by_session_uid(trainable_solver.last_session_uid)\n",
    "\n",
    "print(f\"âœ… Retrieved {len(traces)} trace(s)\\n\")\n",
    "\n",
    "# Inspect the trace\n",
    "trace = traces[0]\n",
    "data = trace.data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRACE DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {data['model']}\")\n",
    "print(f\"Latency: {data['latency_ms']:.2f} ms\")\n",
    "print(f\"Tokens: {data['tokens']['total']} (prompt: {data['tokens']['prompt']}, completion: {data['tokens']['completion']})\")\n",
    "print(f\"\\nInput Messages:\")\n",
    "for msg in data['input']['messages']:\n",
    "    print(f\"  [{msg['role']}]: {msg['content']}\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  {data['output']['choices'][0]['message']['content']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ’¡ This trace contains everything you need for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Rewards and Train\n",
    "\n",
    "Now you can add rewards and use this for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run multiple problems and collect traces\n",
    "problems = [\n",
    "    {\"question\": \"What is 15 + 27?\", \"answer\": \"42\"},\n",
    "    {\"question\": \"What is 8 * 7?\", \"answer\": \"56\"},\n",
    "    {\"question\": \"What is 100 - 37?\", \"answer\": \"63\"},\n",
    "]\n",
    "\n",
    "print(\"Running rollouts...\\n\")\n",
    "\n",
    "session_uids = []\n",
    "for i, problem in enumerate(problems, 1):\n",
    "    result = await trainable_solver.solve(problem[\"question\"])\n",
    "    session_uids.append(trainable_solver.last_session_uid)\n",
    "    \n",
    "    # Simple reward: check if answer is in response\n",
    "    reward = 1.0 if problem[\"answer\"] in result else 0.0\n",
    "    \n",
    "    print(f\"Problem {i}: {problem['question']}\")\n",
    "    print(f\"  Response: {result[:100]}...\" if len(result) > 100 else f\"  Response: {result}\")\n",
    "    print(f\"  Reward: {'âœ“' if reward == 1.0 else 'âœ—'} {reward}\")\n",
    "    print()\n",
    "\n",
    "print(f\"âœ… Collected {len(session_uids)} rollouts\")\n",
    "print(f\"âœ… All traces stored in {DB_PATH}\")\n",
    "print(f\"\\nðŸ’¡ You can now use these traces for:\")\n",
    "print(\"   - Supervised Fine-Tuning (SFT)\")\n",
    "print(\"   - Reinforcement Learning (PPO)\")\n",
    "print(\"   - Debugging and analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Using @trajectory Decorator (Even Easier!)\n",
    "\n",
    "For even cleaner code, use the `@trajectory` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllm.sdk import trajectory\n",
    "\n",
    "class DecoratedMathSolver:\n",
    "    \"\"\"Math solver using @trajectory decorator - cleanest version!\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, model: str = \"gpt-4o-mini\"):\n",
    "        self.client = get_chat_client_async(base_url=base_url, api_key=\"EMPTY\")\n",
    "        self.model = model\n",
    "    \n",
    "    @trajectory(name=\"solver\")  # Automatically creates session and tracks!\n",
    "    async def solve(self, problem: str) -> str:\n",
    "        \"\"\"Solve a math problem.\"\"\"\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Solve this problem: {problem}\"}\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "# Use it\n",
    "decorated_solver = DecoratedMathSolver(base_url=proxy_url)\n",
    "traj = await decorated_solver.solve(\"What is 12 * 8?\")\n",
    "\n",
    "print(\"Decorator Version:\")\n",
    "print(f\"  Result: {traj.result}\")\n",
    "print(f\"  Trajectory Name: {traj.name}\")\n",
    "print(f\"  Number of Steps: {len(traj.steps)}\")\n",
    "print(f\"  Step 0 Trace ID: {traj.steps[0].id}\")\n",
    "print(\"\\nðŸ’¡ The @trajectory decorator automatically returns a TrajectoryView!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: From Code to Training in 3 Steps\n",
    "\n",
    "### 1. Start Proxy (once)\n",
    "```python\n",
    "proxy_manager = ProxyManager(proxy_port=4000)\n",
    "proxy_manager.start_proxy_subprocess(config=config, db_path=\"traces.db\")\n",
    "```\n",
    "\n",
    "### 2. Replace Client (2 lines)\n",
    "```python\n",
    "# Before:\n",
    "from openai import AsyncOpenAI\n",
    "client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "# After:\n",
    "from rllm.sdk import get_chat_client_async, session\n",
    "client = get_chat_client_async(base_url=proxy_url, api_key=\"EMPTY\")\n",
    "```\n",
    "\n",
    "### 3. Wrap in Session\n",
    "```python\n",
    "with session(task=\"my_task\") as sess:\n",
    "    response = await client.chat.completions.create(...)  # Automatically tracked!\n",
    "```\n",
    "\n",
    "### Retrieve Traces\n",
    "```python\n",
    "# By session UID\n",
    "traces = await store.get_by_session_uid(sess._uid)\n",
    "```\n",
    "\n",
    "**That's it!** All your agent interactions are now:\n",
    "- âœ… Fully traced (inputs, outputs, tokens, latency)\n",
    "- âœ… Stored in SQLite\n",
    "- âœ… Ready for training (SFT, PPO)\n",
    "- âœ… Retrievable for debugging and analysis\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Training:** See `tutorial.ipynb` for full SFT and PPO training examples\n",
    "- **Multi-Agent:** Check `examples/sdk/solver_judge/` for complex workflows\n",
    "- **Production:** See `examples/sdk/simple_math/` for complete training scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "proxy_manager.shutdown_proxy()\n",
    "print(\"âœ“ Proxy shutdown complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
