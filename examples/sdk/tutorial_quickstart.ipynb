{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RLLM SDK Quick Start: Make Any Agent Trainable With Almost No Adaptation\n\nThis tutorial shows how to make **any existing agent code** trainable with minimal changes.\n\n**The key insight:** Just replace your OpenAI client with the SDK client, and everything is automatically tracked for training!\n\n*Note: This tutorial focuses on showing you the mechanics. We'll explain how training works at the end.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Start the Proxy\n",
    "\n",
    "One-time setup - start the proxy manager that tracks all LLM calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from rllm.sdk.proxy.proxy_manager import ProxyManager\n",
    "\n",
    "# Setup\n",
    "DB_PATH = \"/tmp/rllm_demo.db\"\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "openai_api_key = \"sk-xxx\"  # Replace with your OpenAI API key\n",
    "\n",
    "# Clean up\n",
    "Path(DB_PATH).unlink(missing_ok=True)\n",
    "\n",
    "# Start proxy\n",
    "proxy_manager = ProxyManager(proxy_port=4000, admin_token=\"my-shared-secret\")\n",
    "config = {\n",
    "    \"model_list\": [\n",
    "        {\n",
    "            \"model_name\": MODEL,\n",
    "            \"litellm_params\": {\n",
    "                \"model\": MODEL,\n",
    "                \"api_key\": openai_api_key,\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "proxy_manager.start_proxy_subprocess(config=config, db_path=DB_PATH, project=\"demo\")\n",
    "proxy_url = proxy_manager.get_proxy_url(include_v1=True)\n",
    "\n",
    "print(f\"âœ“ Proxy started at {proxy_url}\")\n",
    "print(f\"âœ“ Database: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the gsm8k dataset\n",
    "!cd ~/rllm && python -m examples.simple_math.prepare_math_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\"question\": \"15 + 27 * 5?\", \"ground_truth\": \"150\"},\n",
    "    {\"question\": \"If you have 10 apples and you buy 5 more, how many apples do you have in total?\", \"ground_truth\": \"15\"},\n",
    "    {\"question\": \"What is 9 multiplied by 6?\", \"ground_truth\": \"54\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Your Original Agent Code\n",
    "\n",
    "Here's a typical agent using the standard OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import re\n",
    "\n",
    "judge_prompt = \"\"\"You are an expert verifier. Given a countdown problem and multiple solution attempts, select a correct solution.\n",
    "Problem:\n",
    "{problem}\n",
    "Solutions to evaluate:\n",
    "{solutions}\n",
    "A correct solution must satisfy the following criteria:\n",
    "1. The solution uses only the given numbers.\n",
    "2. Each number is used exactly once.\n",
    "3. Only basic arithmetic operations (+, -, *, /) are used.\n",
    "4. The calculation results in the target number.\n",
    "5. The final answer is clearly marked within <answer>...</answer> tags.\n",
    "Output the index of your selected solution within <answer>...</answer> tags, e.g., <answer>1</answer> for the first solution, <answer>2</answer> for the second solution, etc. If multiple solutions are correct, output the index of the first correct solution.\"\"\"\n",
    "\n",
    "\n",
    "class MathAgent:\n",
    "    \"\"\"A simple math solving agent - ORIGINAL VERSION\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        # Standard OpenAI client\n",
    "        self.client = AsyncOpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    async def solve(self, problem: str) -> str:\n",
    "        \"\"\"Solve a math problem.\"\"\"\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{problem}. Output the final answer within <answer>...</answer>\"}],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return self.parse_solver_answer(response.choices[0].message.content)\n",
    "\n",
    "    async def judge(self, problem, solutions: list[str]) -> str:\n",
    "        \"\"\"Judge multiple solutions to a problem.\"\"\"\n",
    "        formatted_solutions = \"\\n\".join([f\"Solution {i + 1}:\\n{sol}\\n\" for i, sol in enumerate(solutions)])\n",
    "        prompt = judge_prompt.format(problem=problem, solutions=formatted_solutions)\n",
    "\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def parse_solver_answer(self, solution):\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", solution, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            return \"<answer>\" + answer_match.group(1).strip() + \"</answer>\"\n",
    "        return \"No solution found\"\n",
    "\n",
    "    def parse_selected_solution(self, judgment, solutions):\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", judgment, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            answer_text = answer_match.group(1).strip()\n",
    "            try:\n",
    "                solution_index = int(answer_text)\n",
    "                return solutions[solution_index - 1]\n",
    "            except (ValueError, IndexError):\n",
    "                return \"\"\n",
    "        return \"\"\n",
    "\n",
    "    async def run(self, problem: str, n_solutions: int = 2) -> str:\n",
    "        \"\"\"Generate multiple solutions and judge them.\"\"\"\n",
    "        solutions = []\n",
    "        for i in range(n_solutions):\n",
    "            sol = await self.solve(problem)\n",
    "            solutions.append(sol)\n",
    "\n",
    "        judgment = await self.judge(problem, solutions)\n",
    "        selected_solution = self.parse_selected_solution(judgment, solutions)\n",
    "        return selected_solution\n",
    "\n",
    "\n",
    "# Use it\n",
    "agent = MathAgent(api_key=openai_api_key, model=MODEL)\n",
    "result = await agent.run(dataset[0][\"question\"])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Make It Trainable (2 Simple Changes!)\n\n**Change 1:** Import the SDK client instead of OpenAI client  \n**Change 2:** Point to the proxy URL\n\nWrap your agent run in `with session()` to group all LLM calls together. Use `sess._uid` to retrieve traces later."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllm.sdk import get_chat_client_async, session  # Change 1: Import SDK client\n",
    "import re\n",
    "\n",
    "\n",
    "class TrainableMathAgent:\n",
    "    \"\"\"A simple math solving agent - ORIGINAL VERSION\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        # Replace standard OpenAI client with SDK client\n",
    "        # self.client = AsyncOpenAI(api_key=api_key)\n",
    "        self.client = get_chat_client_async(api_key=api_key, base_url=proxy_url)\n",
    "        self.model = model\n",
    "\n",
    "    async def solve(self, problem: str) -> str:\n",
    "        \"\"\"Solve a math problem.\"\"\"\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{problem}. Output the final answer within <answer>...</answer>\"}],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return self.parse_solver_answer(response.choices[0].message.content)\n",
    "\n",
    "    async def judge(self, problem, solutions: list[str]) -> str:\n",
    "        \"\"\"Judge multiple solutions to a problem.\"\"\"\n",
    "        formatted_solutions = \"\\n\".join([f\"Solution {i + 1}:\\n{sol}\\n\" for i, sol in enumerate(solutions)])\n",
    "        prompt = judge_prompt.format(problem=problem, solutions=formatted_solutions)\n",
    "\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return self.parse_solver_answer(response.choices[0].message.content)\n",
    "\n",
    "    def parse_solver_answer(self, solution):\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", solution, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            return \"<answer>\" + answer_match.group(1).strip() + \"</answer>\"\n",
    "        return \"No solution found\"\n",
    "\n",
    "    def parse_selected_solution(self, judgment, solutions):\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", judgment, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            answer_text = answer_match.group(1).strip()\n",
    "            try:\n",
    "                solution_index = int(answer_text)\n",
    "                return solutions[solution_index - 1]\n",
    "            except (ValueError, IndexError):\n",
    "                return \"\"\n",
    "        return \"\"\n",
    "\n",
    "    async def run(self, problem: str, n_solutions: int = 2) -> str:\n",
    "        \"\"\"Generate multiple solutions and judge them.\"\"\"\n",
    "        solutions = []\n",
    "        for _ in range(n_solutions):\n",
    "            sol = await self.solve(problem)\n",
    "            solutions.append(sol)\n",
    "\n",
    "        judgment = await self.judge(problem, solutions)\n",
    "        selected_solution = self.parse_selected_solution(judgment, solutions)\n",
    "        return selected_solution\n",
    "\n",
    "\n",
    "# Use it\n",
    "agent = TrainableMathAgent(api_key=openai_api_key, model=MODEL)\n",
    "with session() as sess:\n",
    "    result = await agent.run(dataset[0][\"question\"])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Retrieve the Traces\n\nUse `sess._uid` to retrieve all traces from that session:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllm.sdk.store import SqliteTraceStore\n",
    "\n",
    "# Flush to ensure traces are written\n",
    "await proxy_manager.flush_tracer(timeout=10.0)\n",
    "\n",
    "# Retrieve traces by session UID\n",
    "store = SqliteTraceStore(db_path=DB_PATH)\n",
    "traces = await store.get_by_session_uid(sess._uid)\n",
    "\n",
    "print(f\"âœ… Retrieved {len(traces)} trace(s)\\n\")\n",
    "\n",
    "# Inspect the trace\n",
    "trace = traces[0]\n",
    "data = trace.data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRACE DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {data['model']}\")\n",
    "print(f\"Latency: {data['latency_ms']:.2f} ms\")\n",
    "print(f\"Tokens: {data['tokens']['total']} (prompt: {data['tokens']['prompt']}, completion: {data['tokens']['completion']})\")\n",
    "print(f\"\\nInput Messages:\")\n",
    "for msg in data[\"input\"][\"messages\"]:\n",
    "    print(f\"  [{msg['role']}]: {msg['content']}\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  {data['output']['choices'][0]['message']['content']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ’¡ This trace contains everything you need for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Rewards and Train\n",
    "\n",
    "Now you can add rewards and use this for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response: str) -> str:\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", response, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def reward_fn(response: str, answer: str) -> float:\n",
    "    extracted_answer = extract_answer(response)\n",
    "    return 1.0 if extracted_answer == answer else 0.0\n",
    "\n",
    "\n",
    "async def rollout_v1(question: str, ground_truth: str, **kwargs) -> float:\n",
    "    # we need to provide an rollout function that return a reward\n",
    "    agent = TrainableMathAgent(api_key=openai_api_key, model=\"Qwen/Qwen3-4B-Instruct-2507\")\n",
    "    response = await agent.run(question)\n",
    "    reward = reward_fn(response, ground_truth)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bonus: Using @trajectory Decorator for Step-Level Control\n\nThe `@trajectory` decorator is **equivalent to `with session()`** - both group LLM calls using contextvar.\n\n**Key difference:** `@trajectory` returns a `TrajectoryView` object with a `.steps` attribute, giving you fine-grained control.\n\n**When to use which:**\n- `with session()`: Episode-level rewards (one reward for the whole episode)\n- `@trajectory`: Step-level rewards (assign different rewards to solver vs judge)\n\nAccess `traj.steps` to assign rewards to individual steps:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllm.sdk import trajectory\n",
    "from rllm.sdk.protocol import TrajectoryView\n",
    "\n",
    "\n",
    "class DecoratorMathAgent:\n",
    "    \"\"\"A simple math solving agent - ORIGINAL VERSION\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        # Replace standard OpenAI client with SDK client\n",
    "        # self.client = AsyncOpenAI(api_key=api_key)\n",
    "        self.client = get_chat_client_async(api_key=api_key, base_url=proxy_url)\n",
    "        self.model = model\n",
    "\n",
    "    @trajectory(name=\"solver\")\n",
    "    async def solve(self, problem: str) -> str:\n",
    "        \"\"\"Solve a math problem.\"\"\"\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"{problem}. Output the final answer within <answer>...</answer>\"}],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @trajectory(name=\"judge\")\n",
    "    async def judge(self, problem, solutions: list[str]) -> str:\n",
    "        \"\"\"Judge multiple solutions to a problem.\"\"\"\n",
    "        formatted_solutions = \"\\n\".join([f\"Solution {i + 1}:\\n{sol}\\n\" for i, sol in enumerate(solutions)])\n",
    "        prompt = judge_prompt.format(problem=problem, solutions=formatted_solutions)\n",
    "\n",
    "        response = await self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return self.parse_solver_answer(response.choices[0].message.content)\n",
    "\n",
    "    def parse_solver_answer(self, solution):\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", solution, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            return \"<answer>\" + answer_match.group(1).strip() + \"</answer>\"\n",
    "        return \"No solution found\"\n",
    "\n",
    "    def parse_selected_solution(self, judgment, solutions):\n",
    "        answer_match = re.search(r\"<answer>(.*?)</answer>\", judgment, re.IGNORECASE | re.DOTALL)\n",
    "        if answer_match:\n",
    "            answer_text = answer_match.group(1).strip()\n",
    "            try:\n",
    "                solution_index = int(answer_text)\n",
    "                return solutions[solution_index - 1]\n",
    "            except (ValueError, IndexError):\n",
    "                return \"\"\n",
    "        return \"\"\n",
    "\n",
    "    async def run(self, problem: str, n_solutions: int = 2, ground_truth: str = None) -> str:\n",
    "        \"\"\"Generate multiple solutions and judge them.\"\"\"\n",
    "        solutions = []\n",
    "        for _ in range(n_solutions):\n",
    "            sol = await self.solve(problem)\n",
    "            solutions.append(sol)\n",
    "\n",
    "        judgment = await self.judge(problem, solutions)\n",
    "        selected_solution = self.parse_selected_solution(judgment.result, solutions)\n",
    "\n",
    "        # assign reward for each step in trajectory\n",
    "        for sol in solutions:\n",
    "            sol.reward = reward_fn(sol.result, ground_truth)\n",
    "            sol.steps[0].reward = sol.reward\n",
    "\n",
    "        judgment.reward = reward_fn(selected_solution, ground_truth)\n",
    "        judgment.steps[0].reward = judgment.reward\n",
    "\n",
    "        return solutions + [judgment]\n",
    "\n",
    "\n",
    "# Use it\n",
    "async def rollout_v2(question: str, ground_truth: str, **kwargs) -> list[TrajectoryView]:\n",
    "    agent = DecoratorMathAgent(None, model=\"Qwen/Qwen3-4B-Instruct-2507\")\n",
    "    trajs = await agent.run(question, ground_truth=ground_truth)\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "from rllm.trainer import AgentTrainer\n",
    "from rllm.data.dataset import DatasetRegistry\n",
    "from hydra import initialize_config_dir, compose\n",
    "import os\n",
    "\n",
    "config_dir = os.path.abspath(\".\")\n",
    "with initialize_config_dir(config_dir=config_dir, version_base=None):\n",
    "    config = compose(config_name=\"tutorial_config\")\n",
    "\n",
    "dataset = DatasetRegistry.load_dataset(\"hendrycks_math\", \"train\")\n",
    "trainer = AgentTrainer(\n",
    "    agent_run_func=rollout_v1,  # rollout_v2\n",
    "    config=config,\n",
    "    train_dataset=dataset,\n",
    "    val_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## How Does Training Work?\n\nNow that you've seen the mechanics, here's what happens under the hood:\n\n1. **Trace Collection:** The proxy captures all LLM calls as traces (inputs, outputs, tokens, latency)\n2. **Reward Assignment:** You provide rewards to tell the model what's good (correct answer = 1.0, wrong = 0.0)\n3. **Training Loop:** The trainer feeds traces + rewards to the model\n4. **Learning:** The model adjusts its weights to maximize rewards - producing more behaviors that get high rewards\n5. **Improvement:** Over time, the model learns from experience and gets better at the task\n\nThis is reinforcement learning: the model tries different approaches, gets feedback (rewards), and learns what works.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Design Details (For The Curious)\n\n**Why a proxy?**  \nTransparent interception of LLM calls without modifying your agent code. Works with any OpenAI-compatible API.\n\n**How does session tracking work?**  \nUses Python's **contextvar** for automatic context propagation. When you use `with session()` or `@trajectory`, it creates a context. All LLM calls inside that context are automatically grouped together. Thread-safe, no manual tracking needed.\n\n**Session vs Trajectory:**  \nBoth create tracking contexts using contextvar under the hood.\n- `with session()`: Simple, returns session object with `._uid` for retrieval. Good for episode-level rewards.\n- `@trajectory`: Returns `TrajectoryView` with `.steps` attribute for fine-grained control. Use when you need step-level rewards.\n\n**Why SQLite storage?**  \nEnables offline training with no dependencies on live services. Query and analyze traces anytime. Portable and lightweight.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "proxy_manager.shutdown_proxy()\n",
    "print(\"âœ“ Proxy shutdown complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllm-1103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}