{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RLLM SDK Quick Start: Make Any Agent Trainable With Almost No Adaptation\n\nThis tutorial shows how to make **any existing agent code** trainable with minimal changes.\n\n**The key insight:** Just replace your OpenAI client with the SDK client, and everything is automatically tracked for training!\n\n*Note: This tutorial focuses on showing you the mechanics. We'll explain how training works at the end.*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Start the Proxy\n\nStart the proxy for testing. During training, the Trainer manages this automatically. The proxy logs all LLM calls to a SQLite database."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from rllm.sdk.proxy.proxy_manager import ProxyManager\n",
    "\n",
    "# Setup\n",
    "DB_PATH = \"/tmp/rllm_demo.db\"\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "openai_api_key = \"sk-xxx\"  # Replace with your OpenAI API key\n",
    "\n",
    "# Clean up\n",
    "Path(DB_PATH).unlink(missing_ok=True)\n",
    "\n",
    "# Start proxy\n",
    "proxy_manager = ProxyManager(proxy_port=4000, admin_token=\"my-shared-secret\")\n",
    "config = {\n",
    "    \"model_list\": [\n",
    "        {\n",
    "            \"model_name\": MODEL,\n",
    "            \"litellm_params\": {\n",
    "                \"model\": MODEL,\n",
    "                \"api_key\": openai_api_key,\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "proxy_manager.start_proxy_subprocess(config=config, db_path=DB_PATH, project=\"demo\")\n",
    "proxy_url = proxy_manager.get_proxy_url(include_v1=True)\n",
    "\n",
    "print(f\"âœ“ Proxy started at {proxy_url}\")\n",
    "print(f\"âœ“ Database: {DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from rllm.data.dataset import DatasetRegistry\n\ntrain_dataset = DatasetRegistry.load_dataset(\"countdown\", \"train\")\ntest_dataset = DatasetRegistry.load_dataset(\"countdown\", \"test\")\n\ntrain_dataset[0]"
  },
  {
   "cell_type": "markdown",
   "source": "**The Countdown Task:**  \nGiven a set of numbers and a target, find an arithmetic expression using those numbers to reach the target. Each number can be used at most once. For example: numbers `[30, 32, 76]` and target `78` â†’ solution could be `76 + 32 - 30 = 78`.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Step 2: Your Original Agent Code\n\nA typical agent using the standard OpenAI client. This agent follows a Solver-Judge workflow: generate multiple solution attempts, then select the best one."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Make It Trainable (2 Simple Changes!)\n\n**Change 1:** Import the SDK client instead of OpenAI client  \n**Change 2:** Point to the proxy URL\n\n**What's `session()`?** A lightweight primitive that tracks all LLM calls within its scope and injects metadata into each call. Everything inside `with session()` is automatically grouped and retrievable via `sess._uid`."
  },
  {
   "cell_type": "code",
   "source": "from openai import AsyncOpenAI\nimport re\n\njudge_prompt = \"\"\"You are an expert verifier. Given a countdown problem and multiple solution attempts, select a correct solution.\nProblem:\n{problem}\nSolutions to evaluate:\n{solutions}\nA correct solution must satisfy the following criteria:\n1. The solution uses only the given numbers.\n2. Each number is used exactly once.\n3. Only basic arithmetic operations (+, -, *, /) are used.\n4. The calculation results in the target number.\n5. The final answer is clearly marked within <answer>...</answer> tags.\nOutput the index of your selected solution within <answer>...</answer> tags, e.g., <answer>1</answer> for the first solution, <answer>2</answer> for the second solution, etc. If multiple solutions are correct, output the index of the first correct solution.\"\"\"\n\n\nclass CountdownAgent:\n    \"\"\"A simple math solving agent - ORIGINAL VERSION\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n        # Standard OpenAI client\n        self.client = AsyncOpenAI(api_key=api_key)\n        self.model = model\n\n    async def solve(self, problem: str) -> str:\n        \"\"\"Solve a math problem.\"\"\"\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": f\"{problem}. Output the final answer within <answer>...</answer>\"}],\n            max_tokens=1000,\n        )\n        return self.parse_solver_answer(response.choices[0].message.content)\n\n    async def judge(self, problem, solutions: list[str]) -> str:\n        \"\"\"Judge multiple solutions to a problem.\"\"\"\n        formatted_solutions = \"\\n\".join([f\"Solution {i + 1}:\\n{sol}\\n\" for i, sol in enumerate(solutions)])\n        prompt = judge_prompt.format(problem=problem, solutions=formatted_solutions)\n\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=1000,\n        )\n        return response.choices[0].message.content\n\n    def parse_solver_answer(self, solution):\n        # Find all <answer> tags and return the last one\n        answer_matches = re.findall(r\"<answer>(.*?)</answer>\", solution, re.IGNORECASE | re.DOTALL)\n        if answer_matches:\n            return \"<answer>\" + answer_matches[-1].strip() + \"</answer>\"\n        return \"No solution found\"\n\n    def parse_selected_solution(self, judgment, solutions):\n        # Find all <answer> tags and use the last one\n        answer_matches = re.findall(r\"<answer>(.*?)</answer>\", judgment, re.IGNORECASE | re.DOTALL)\n        if answer_matches:\n            answer_text = answer_matches[-1].strip()\n            try:\n                solution_index = int(answer_text)\n                return solutions[solution_index - 1]\n            except (ValueError, IndexError):\n                return \"\"\n        return \"\"\n\n    async def run(self, problem: str, n_solutions: int = 2) -> str:\n        \"\"\"Generate multiple solutions and judge them.\"\"\"\n        solutions = []\n        for i in range(n_solutions):\n            sol = await self.solve(problem)\n            solutions.append(sol)\n\n        judgment = await self.judge(problem, solutions)\n        # print(judgment)\n        selected_solution = self.parse_selected_solution(judgment, solutions)\n        # print(f\"Selected solution:\\n{selected_solution}\")\n        return selected_solution\n\n\n# Use it\nagent = CountdownAgent(api_key=openai_api_key, model=MODEL)\nresult = await agent.run(train_dataset[0][\"question\"])\n\nprint(result)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from rllm.sdk import get_chat_client_async, session  # Change 1: Import SDK client\nimport re\n\n\nclass TrainableAgent:\n    \"\"\"A simple math solving agent - TRAINABLE VERSION\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n        # Replace standard OpenAI client with SDK client\n        # self.client = AsyncOpenAI(api_key=api_key)\n        self.client = get_chat_client_async(api_key=api_key, base_url=proxy_url)\n        self.model = model\n\n    async def solve(self, problem: str) -> str:\n        \"\"\"Solve a math problem.\"\"\"\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": f\"{problem}. Output the final answer within <answer>...</answer>\"}],\n            max_tokens=1000,\n        )\n        return self.parse_solver_answer(response.choices[0].message.content)\n\n    async def judge(self, problem, solutions: list[str]) -> str:\n        \"\"\"Judge multiple solutions to a problem.\"\"\"\n        formatted_solutions = \"\\n\".join([f\"Solution {i + 1}:\\n{sol}\\n\" for i, sol in enumerate(solutions)])\n        prompt = judge_prompt.format(problem=problem, solutions=formatted_solutions)\n\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=1000,\n        )\n        return self.parse_solver_answer(response.choices[0].message.content)\n\n    def parse_solver_answer(self, solution):\n        # Find all <answer> tags and return the last one\n        answer_matches = re.findall(r\"<answer>(.*?)</answer>\", solution, re.IGNORECASE | re.DOTALL)\n        if answer_matches:\n            return \"<answer>\" + answer_matches[-1].strip() + \"</answer>\"\n        return \"No solution found\"\n\n    def parse_selected_solution(self, judgment, solutions):\n        # Find all <answer> tags and use the last one\n        answer_matches = re.findall(r\"<answer>(.*?)</answer>\", judgment, re.IGNORECASE | re.DOTALL)\n        if answer_matches:\n            answer_text = answer_matches[-1].strip()\n            try:\n                solution_index = int(answer_text)\n                return solutions[solution_index - 1]\n            except (ValueError, IndexError):\n                return \"\"\n        return \"\"\n\n    async def run(self, problem: str, n_solutions: int = 2) -> str:\n        \"\"\"Generate multiple solutions and judge them.\"\"\"\n        solutions = []\n        for _ in range(n_solutions):\n            sol = await self.solve(problem)\n            solutions.append(sol)\n\n        judgment = await self.judge(problem, solutions)\n        selected_solution = self.parse_selected_solution(judgment, solutions)\n        return selected_solution\n\n\n# # Use it\nagent = TrainableAgent(api_key=openai_api_key, model=MODEL)\nwith session() as sess:\n    result = await agent.run(train_dataset[0][\"question\"])\n\nprint(result)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Why This Makes It Trainable: Automatic LLM Call Tracking\n\nThe `session()` primitive enables training by capturing every LLM interaction. Let's verify by retrieving the traces:\n\nEach trace contains:\n- `input`: Prompt messages sent to the model\n- `output`: Model's response\n- `tokens`: Exact token IDs (ensures correctness, bypasses retokenization issues)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllm.sdk.store import SqliteTraceStore\n",
    "\n",
    "# Flush to ensure traces are written\n",
    "await proxy_manager.flush_tracer(timeout=10.0)\n",
    "\n",
    "# Retrieve traces by session UID\n",
    "store = SqliteTraceStore(db_path=DB_PATH)\n",
    "traces = await store.get_by_session_uid(sess._uid)\n",
    "\n",
    "print(f\"âœ… Retrieved {len(traces)} trace(s)\\n\")\n",
    "\n",
    "# Inspect the trace\n",
    "trace = traces[0]\n",
    "data = trace.data\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRACE DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: {data['model']}\")\n",
    "print(f\"Latency: {data['latency_ms']:.2f} ms\")\n",
    "print(f\"Tokens: {data['tokens']['total']} (prompt: {data['tokens']['prompt']}, completion: {data['tokens']['completion']})\")\n",
    "print(f\"\\nInput Messages:\")\n",
    "for msg in data[\"input\"][\"messages\"]:\n",
    "    print(f\"  [{msg['role']}]: {msg['content']}\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  {data['output']['choices'][0]['message']['content']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ’¡ This trace contains everything you need for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Add Rewards and Train\n\nDefine a reward function that scores agent outputs, then pass it to the trainer:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_answer(response: str) -> str:\n    match = re.search(r\"<answer>(.*?)</answer>\", response, re.IGNORECASE | re.DOTALL)\n    if match:\n        return match.group(1).strip()\n    return \"\"\n\n\ndef reward_fn(response: str, answer: str) -> float:\n    extracted_answer = extract_answer(response)\n    return 1.0 if answer in extracted_answer else 0.0\n\n\nasync def rollout_v1(question: str, ground_truth: str, **kwargs) -> float:\n    # we need to provide an rollout function that return a reward\n    agent = TrainableAgent(api_key=openai_api_key, model=\"Qwen/Qwen3-4B-Instruct-2507\")\n    response = await agent.run(question)\n    reward = reward_fn(response, ground_truth)\n    return reward"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bonus: Using @trajectory Decorator for Step-Level Control\n\nThe `@trajectory` decorator is **equivalent to `with session()`** - both track LLM calls using contextvar.\n\n**Key difference:** Both provide `.steps` access for fine-grained control:\n- `with session() as sess:` â†’ `sess.steps` \n- `@trajectory(name=\"...\")` â†’ returns `TrajectoryView` with `.steps`\n\n**When to use:**\n- `with session()`: Simple episode tracking\n- `@trajectory`: Multi-step agents where you want explicit step-level rewards (e.g., reward solver differently from judge)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from rllm.sdk import trajectory\nfrom rllm.sdk.protocol import TrajectoryView\n\n\nclass TrainableAgentV2:\n    \"\"\"A simple math solving agent - TRAINABLE VERSION V2\"\"\"\n\n    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n        # Replace standard OpenAI client with SDK client\n        # self.client = AsyncOpenAI(api_key=api_key)\n        self.client = get_chat_client_async(api_key=api_key, base_url=proxy_url)\n        self.model = model\n\n    @trajectory(name=\"solver\")\n    async def solve(self, problem: str) -> str:\n        \"\"\"Solve a math problem.\"\"\"\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": f\"{problem}. Output the final answer within <answer>...</answer>\"}],\n            max_tokens=1000,\n        )\n        return response.choices[0].message.content\n\n    @trajectory(name=\"judge\")\n    async def judge(self, problem, solutions: list[str]) -> str:\n        \"\"\"Judge multiple solutions to a problem.\"\"\"\n        formatted_solutions = \"\\n\".join([f\"Solution {i + 1}:\\n{sol}\\n\" for i, sol in enumerate(solutions)])\n        prompt = judge_prompt.format(problem=problem, solutions=formatted_solutions)\n\n        response = await self.client.chat.completions.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=1000,\n        )\n        return self.parse_solver_answer(response.choices[0].message.content)\n\n    def parse_solver_answer(self, solution):\n        # Find all <answer> tags and return the last one\n        answer_matches = re.findall(r\"<answer>(.*?)</answer>\", solution, re.IGNORECASE | re.DOTALL)\n        if answer_matches:\n            return \"<answer>\" + answer_matches[-1].strip() + \"</answer>\"\n        return \"No solution found\"\n\n    def parse_selected_solution(self, judgment, solutions):\n        # Find all <answer> tags and use the last one\n        answer_matches = re.findall(r\"<answer>(.*?)</answer>\", judgment, re.IGNORECASE | re.DOTALL)\n        if answer_matches:\n            answer_text = answer_matches[-1].strip()\n            try:\n                solution_index = int(answer_text)\n                return solutions[solution_index - 1]\n            except (ValueError, IndexError):\n                return \"\"\n        return \"\"\n\n    async def run(self, problem: str, n_solutions: int = 2, ground_truth: str = None) -> str:\n        \"\"\"Generate multiple solutions and judge them.\"\"\"\n        solutions = []\n        for _ in range(n_solutions):\n            sol = await self.solve(problem)\n            solutions.append(sol)\n\n        judgment = await self.judge(problem, solutions)\n        selected_solution = self.parse_selected_solution(judgment.result, solutions)\n\n        # assign reward for each step in trajectory\n        for sol in solutions:\n            sol.reward = reward_fn(sol.result, ground_truth)\n            sol.steps[0].reward = sol.reward\n\n        judgment.reward = reward_fn(selected_solution, ground_truth)\n        judgment.steps[0].reward = judgment.reward\n\n        return solutions + [judgment]\n\n\n# Use it\nasync def rollout_v2(question: str, ground_truth: str, **kwargs) -> list[TrajectoryView]:\n    agent = TrainableAgentV2(None, model=\"Qwen/Qwen3-4B-Instruct-2507\")\n    trajs = await agent.run(question, ground_truth=ground_truth)\n    return trajs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training\nfrom rllm.trainer import AgentTrainer\nfrom hydra import initialize_config_dir, compose\nimport os\n\nwith initialize_config_dir(config_dir=os.path.abspath(\".\"), version_base=None):\n    config = compose(config_name=\"tutorial_config\")\n\ntrainer = AgentTrainer(\n    agent_run_func=rollout_v1,  # rollout_v2\n    config=config,\n    train_dataset=train_dataset,\n    val_dataset=test_dataset,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## How Does Training Work?\n\nHere's what happens under the hood:\n\n1. **Trace Collection:** The proxy captures all LLM calls (inputs, outputs, tokens, latency)\n2. **Reward Assignment:** You define what's good (correct answer = 1.0, wrong = 0.0)\n3. **Training Loop:** The trainer feeds traces + rewards to the model\n4. **Learning:** The model adjusts weights to maximize rewards\n5. **Improvement:** Over time, the model learns successful behaviors\n\nThis is reinforcement learning: try different approaches, get feedback, learn what works.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Design Details (For The Curious)\n\n**Why a proxy?**  \nTransparent LLM call interception without modifying agent code. Works with any OpenAI-compatible API.\n\n**How does session tracking work?**  \nUses Python's **contextvar** for automatic context propagation. `with session()` or `@trajectory` creates a context that automatically groups all LLM calls inside it. Thread-safe, zero manual tracking.\n\n**Session vs Trajectory:**  \nBoth use contextvar under the hood:\n- `with session()`: Returns session object with `._uid` for retrieval\n- `@trajectory`: Returns `TrajectoryView` with `.steps` for fine-grained control\n\n**Why SQLite storage?**  \nOffline training with no live service dependencies. Query and analyze traces anytime.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "proxy_manager.shutdown_proxy()\n",
    "print(\"âœ“ Proxy shutdown complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllm-1103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}